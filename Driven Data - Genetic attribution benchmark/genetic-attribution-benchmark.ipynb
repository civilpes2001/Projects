{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train_val = pd.read_csv('../input/genetic-engineering-attribution-challenge/train_values.csv', index_col='sequence_id')\ntest_val = pd.read_csv('../input/genetic-engineering-attribution-challenge/test_values.csv', index_col='sequence_id')\nsubmission = pd.read_csv('../input/genetic-engineering-attribution-challenge/submission_format_3TFRxH6.csv')\ntrain_labels = pd.read_csv('../input/genetic-engineering-attribution-challenge/train_labels.csv', index_col='sequence_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_val.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have one non-numeric column, sequence, which contains the DNA plasmid sequence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_val.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The length of these sequences vary. Some are as short as 20 characters and others are as long as 60,000!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_lengths = train_val.sequence.apply(len)\n\nsequence_lengths.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_lengths.plot(\n    kind='hist', \n    title='Distribution of DNA Sequence Lengths', \n    bins=150,\n    xlim=(0, 20000));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exclude the 0th column which is the dna sequence\ntrain_val.iloc[:, 1:].apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since these columns are binary, the mean value shows us the prevalence, meaning what proportion of sequences have that characteristic. Check out the problem description page for definitions of each feature.\n\nThis plot shows us that a few features are common across sequences but most are quite rare.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_binary_features = train_val.iloc[:, 1:].mean().sort_values()\n\nax = sorted_binary_features.plot(kind='barh',\n                                 stacked=True,\n                                 figsize=(5, 12),\n                                 title='Prevalence of Binary Features')\nax.set_xlabel('Proportion of sequences');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test set\n\nOur test set features are set up in the same way as the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_val.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_val.sequence.apply(len).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAINING LABELS\n\nNow let's look at our labels. It's important to recognize that these are one-hot encoded. Each column represents a lab ID. The correct lab for a given sequence_id is indicated by a 1.0 in that column, and 0.0 otherwise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make our modeling and analysis simpler, let's collapse the labels dataframe into one column that shows us the lab ID for each sequence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the column with the max value in each row\nlab_ids = pd.DataFrame(train_labels.idxmax(axis=1), columns=['lab_id'])\nlab_ids.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Labs can have anywhere from 1 to over 8,000 sequences in the data. On average, there are about 48 sequences per lab in the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the distrubtion of lab prevalence in the training set\nlab_ids['lab_id'].value_counts().describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorting labs by prevalence, we see that lab ID I7FXTVDP accounts for 13% of the sequences in the train set. The top 3 labs account for more than 20% of the data. We'll have to be mindful of class imbalance in our training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort lab ids by prevalence\n(lab_ids['lab_id'].value_counts(normalize=True)\n        .sort_values(ascending=False)).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONSTRUCT FEATURES FROM DNA SEQUENCES","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For this benchmark, we're going to use the DNA sequences as the basis for the features in our model. We've got some preprocessing work to do to turn these lengthy strings into useful features!\n\nThese sequences are composed of five characters. G, C, A, and T represent the four nucleotides commonly found in DNA (guanine, cytosine, adenine, thymine). N stand for any nucleotide (not a gap).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bases = set(''.join(train_val.sequence.values))\nbases","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One common way to turn strings into useful features is to count n-grams, or continuous subsequences of length n. Here, we'll split up the DNA sequences into four-grams, or subsequences consisting of 4 bases.\n\nWith 5 unique bases, we can produce 120 different sequence permutations consisting of 4 bases. You can play around with the length of n to see how it affects your model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import permutations\n\nn = 4\nsubsequences = [''.join(permutation) for permutation in permutations(bases, r=n)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of subsequences: {len(subsequences)}\")\nsubsequences[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now turn our strings into features by taking a simple count of each subsequence. We could do this one of two ways:\n\nOverlapping substrings: To count overlapping substrings, we would use a sliding window such that the sequence \"ATTATTA\" will result in a count of 2 for the substring \"ATTA\" (\"ATTA-TTA\" and \"ATT-ATTA\").\n\nNon-overlapping substrings: To count non-overlapping substrings, we search for each substring. If we find it, we count it and then continue our search at the end of the substring. In this case, \"ATTATTA\" will result in a count of 1 for the substring \"ATTA\" (\"ATTA-TTA\").\n\nFor simplicity, we're going to use the second method of non-overlapping substrings. We can use the built in count method on strings. Feel free to try both methods to see how it affects your model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of built-in count method on strings\n# Because it's non-overlapping, \"atta\" is only counted twice\n\"gattattattaca\".count(\"atta\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's create a helper function to generate features. get_ngram_features will create a new dataframe that holds the counts for each subsequence and row in our data.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def get_ngram_features(data, subsequences):\n    \"\"\"Generates counts for each subsequence.\n\n    Args:\n        data (DataFrame): The data you want to create features from. Must include a \"sequence\" column.\n        subsequences (list): A list of subsequences to count.\n\n    Returns:\n        DataFrame: A DataFrame with one column for each subsequence.\n    \"\"\"\n    features = pd.DataFrame(index=data.index)\n    for subseq in subsequences:\n        features[subseq] = data.sequence.str.count(subseq)\n    return features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using our helper function, we can generate feautures for all the sequences in our training set! This will take a minute or two to run.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate n-gram features on our training set\nngram_features = get_ngram_features(train_val, subsequences)\nngram_features.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ngram_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have features for all 120 possible subsequences. Their values show the counts of each 4-gram within the full DNA sequence.\n\nLet's join them with our one-hot endcoded binary features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = ngram_features.join(train_val.drop('sequence', axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THE ERROR METRIC: TOP TEN ACCURACY","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nThe goal for this competition is to help narrow down the field of possible labs-of-origin from thousands to just a few. To that end, predictions will be evaluated based on top-ten accuracy. That means we'll consider a prediction \"correct\" if the true lab-of-origin is in the top ten most likely labs.\n\nThere is not a built in evaluation metric for top-k accuracy in scikit-learn, so we'll be constructing a custom scorer. We'll use this to determine the final accuracy of our model.\n\nFirst, we'll need to create a custom scorer that can take in an estimator, validation data, and labels, and output a score based on the top ten results from each predicton. Scikit-learn let's us do this if we adhere to a specific signature.\n\n**From the documentation:**\n\nFor a callable to be a scorer, it needs to meet the protocol specified by the following two rules:\n\nIt can be called with parameters (estimator, X, y), where estimator is the model that should be evaluated, X is validation data, and y is the ground truth target for X (in the supervised case) or None (in the unsupervised case).\n\nIt returns a floating point number that quantifies the estimator prediction quality on X, with reference to y. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def top10_accuracy_scorer(estimator, X, y):\n    \"\"\"A custom scorer that evaluates a model on whether the correct label is in \n    the top 10 most probable predictions.\n\n    Args:\n        estimator (sklearn estimator): The sklearn model that should be evaluated.\n        X (numpy array): The validation data.\n        y (numpy array): The ground truth labels.\n\n    Returns:\n        float: Accuracy of the model as defined by the proportion of predictions\n               in which the correct label was in the top 10. Higher is better.\n    \"\"\"\n    # predict the probabilities across all possible labels for rows in our training set\n    probas = estimator.predict_proba(X)\n    \n    # get the indices for top 10 predictions for each row; these are the last ten in each row\n    # Note: We use argpartition, which is O(n), vs argsort, which uses the quicksort algorithm \n    # by default and is O(n^2) in the worst case. We can do this because we only need the top ten\n    # partitioned, not in sorted order.\n    # Documentation: https://numpy.org/doc/1.18/reference/generated/numpy.argpartition.html\n    top10_idx = np.argpartition(probas, -10, axis=1)[:, -10:]\n    \n    # index into the classes list using the top ten indices to get the class names\n    top10_preds = estimator.classes_[top10_idx]\n\n    # check if y-true is in top 10 for each set of predictions\n    mask = top10_preds == y.reshape((y.size, 1))\n    \n    # take the mean\n    top_10_accuracy = mask.any(axis=1).mean()\n \n    return top_10_accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BUILD THE MODEL\n\nRandom forests are often a good first model to try so we'll start there.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## RANDOM FOREST\n\nIt's easy to build a random forest model with Scikit Learn. We're going to create a simple model with a few specified hyperparameters.\n\nTip: If you wanted to get fancy, you could perform a cross-validated grid search with GridSearchCV. You could even use the custom scorer we made earlier to train your model by passing it in via the scoring parameter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's generate our labels and rename our features to match.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename our feature array\nX = all_features\n\n# Create our labels\ny = lab_ids.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've got our features and our labels. Time to train!\n\nWait a sec... aren't we forgetting something? Oh right, we still have to address the class imbalance we discovered earlier. Luckily, scikit-learn has an easy solution for us. We can set class_weight to \"balanced\". This will set class weights inversely proportional to the class frequency.\n\nGreat! Now let's generate and fit our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = LGBMClassifier(\n    n_jobs=10, \n    n_estimators=150,\n    class_weight='balanced', \n    max_depth=20, \n    random_state=1230 \n)\n\n# fit our model\nrf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how our model scored on our training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! 65.69% accuracy isn't too shabby for our first try. However, that's using the default scorer, which is vanilla top-1 accuracy. We should expect to do better on the competition metric, top-10 accuracy. Let's use our custom defined scorer to see how we did:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top10_accuracy_scorer(rf, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model got about 83.46% top ten accuracy. Not bad! Now let's make some predictions using our newly trained model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# PREDICT AND SUBMIT\n\nNow we'll create features from our test set and generate predictions. We need to make sure to generate probabilities for each lab ID so that the top 10 accuracy metric can be computed.\n\nFirst, let's create the 4-gram features and join it with the binary features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ngram_features = get_ngram_features(test_val, subsequences)\nall_test_features = test_ngram_features.join(test_val.drop('sequence', axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MAKE PREDICTIONS\n\nIt's important to use predict_proba here, which gives us class probabilities, instead of predict, which would give us class predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = xgb.predict_proba(all_test_features)\n\n# Examine first row\nprobas[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SAVE SUBMISSION\n\nNow let's get our matrix of probabilities ready for submission.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_format = pd.read_csv('../input/genetic-engineering-attribution-challenge/submission_format_3TFRxH6.csv', index_col='sequence_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_format.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we create our submission csv, let's make sure the submission format matches the shape of our predicted probabilities. We're also going to check that the predicted classes are in the same order as the submission format columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"assert submission_format.shape == probas.shape\nassert (xgb.classes_ == submission_format.columns).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame(data=probas, \n                             columns=cat.classes_, \n                             index=submission_format.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good! Let's save and submit!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.to_csv('submission.csv')\nprint('Exported')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}