{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this notebook in Google Colab for exactly reproducible results. (Results may vary on other GPU specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before running this notebook please note that I have GPU Ram Free: 16280 MB, if this is not the case for you in colab, the cell with sequence length 128 and batch size 32 (Model 3) will give memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab Trick: if you run out of memory in colab while using CPU, genrally colab gives you around 25gb cpu and 14 gb gpu ram if your gmail account is old or you have been using gpu from 12 hours. So to get that extra 2 gb GPU which proves useful in most cases all you have to do is run out of memory in a new google colab account :) . Hope every one has atleast two gmail accounts :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: This solution uses roberta-large and bert-large-uncased-whole-word-masking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE TRANSFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27604,
     "status": "ok",
     "timestamp": 1589116331602,
     "user": {
      "displayName": "krishna priya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgWBAV-1HvbztQSHgm8HTqqGtrYwH3abQ0V59zh=s64",
      "userId": "17318029652689759205"
     },
     "user_tz": -330
    },
    "id": "HgnYUh62Q39w",
    "outputId": "60173412-b612-4b07-a654-5c023c6ebb8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 4.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 14.5MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 35.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting tokenizers==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 23.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=55a744544740ab065c1918545ff1902aea755cfe0271a266b340101fbd563288\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n",
      "Collecting simpletransformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/6d/7ca8f05d3c0d38ea2c1f66221701cccf1dbe6720bcf4e5cfaa4644b35ab0/simpletransformers-0.27.2-py3-none-any.whl (170kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 5.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: transformers>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.9.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
      "Collecting seqeval\n",
      "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
      "Collecting tensorboardx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 16.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->simpletransformers) (3.0.12)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->simpletransformers) (0.7)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->simpletransformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->simpletransformers) (0.1.86)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.4.5.1)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.8.0->simpletransformers) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.1.3)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=ad4d48410ee3125468a55c2437c220b4d529ac166258cbb309b98b4f1ff83a09\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval, tensorboardx, simpletransformers\n",
      "Successfully installed seqeval-0.0.12 simpletransformers-0.27.2 tensorboardx-2.0\n",
      "Collecting gputil\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
      "Building wheels for collected packages: gputil\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=396d34d0d99b5ce977478bfc21c68e5be9690f5b6f0eb0b72e4ded762cec4c8e\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
      "Successfully built gputil\n",
      "Installing collected packages: gputil\n",
      "Successfully installed gputil-1.4.0\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install simpletransformers\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1589144347267,
     "user": {
      "displayName": "krishna priya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgWBAV-1HvbztQSHgm8HTqqGtrYwH3abQ0V59zh=s64",
      "userId": "17318029652689759205"
     },
     "user_tz": -330
    },
    "id": "oTtRmVyjRhc2",
    "outputId": "d0183614-4461-46e8-adda-6b10cf3d8bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 12.7 GB  |     Proc size: 156.9 MB\n",
      "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total     16280MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Egu8oM-8Rq44"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "from scipy.special import softmax\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "import sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_all(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8913,
     "status": "ok",
     "timestamp": 1589144357216,
     "user": {
      "displayName": "krishna priya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgWBAV-1HvbztQSHgm8HTqqGtrYwH3abQ0V59zh=s64",
      "userId": "17318029652689759205"
     },
     "user_tz": -330
    },
    "id": "7CcS0mhFRuLa",
    "outputId": "48fd749a-be71-4fe6-ab36-f928ad373748"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5287, 3), (1962, 2), (1962, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/COVID-19 Tweet Classification Challenge/updated_train.csv')\n",
    "test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/COVID-19 Tweet Classification Challenge/updated_test.csv')\n",
    "sample_sub = pd.read_csv('/content/drive/My Drive/Colab Notebooks/COVID-19 Tweet Classification Challenge/updated_ss.csv')\n",
    "train.shape, test.shape, sample_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>The bitcoin halving is cancelled due to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>MercyOfAllah In good times wrapped in its gran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>266 Days No Digital India No Murder of e learn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>India is likely to run out of the remaining RN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>In these tough times the best way to grow is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               text  target\n",
       "0  train_0            The bitcoin halving is cancelled due to       1\n",
       "1  train_1  MercyOfAllah In good times wrapped in its gran...       0\n",
       "2  train_2  266 Days No Digital India No Murder of e learn...       1\n",
       "3  train_3  India is likely to run out of the remaining RN...       1\n",
       "4  train_4  In these tough times the best way to grow is t...       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2</td>\n",
       "      <td>Why is  explained in the video take a look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Ed Davey fasting for Ramadan No contest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4</td>\n",
       "      <td>Is Doja Cat good or do you just miss Nicki Minaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_8</td>\n",
       "      <td>How Boris Johnson s cheery wounded in action p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_9</td>\n",
       "      <td>Man it s terrible Not even a reason to get on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               text\n",
       "0  test_2         Why is  explained in the video take a look\n",
       "1  test_3            Ed Davey fasting for Ramadan No contest\n",
       "2  test_4   Is Doja Cat good or do you just miss Nicki Minaj\n",
       "3  test_8  How Boris Johnson s cheery wounded in action p...\n",
       "4  test_9  Man it s terrible Not even a reason to get on ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8622,
     "status": "ok",
     "timestamp": 1589144357217,
     "user": {
      "displayName": "krishna priya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgWBAV-1HvbztQSHgm8HTqqGtrYwH3abQ0V59zh=s64",
      "userId": "17318029652689759205"
     },
     "user_tz": -330
    },
    "id": "kYe77wJIRxjV",
    "outputId": "7d17a23c-b9b6-4e01-b0db-e26754c3220c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2746\n",
       "1    2541\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8256,
     "status": "ok",
     "timestamp": 1589144357218,
     "user": {
      "displayName": "krishna priya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgWBAV-1HvbztQSHgm8HTqqGtrYwH3abQ0V59zh=s64",
      "userId": "17318029652689759205"
     },
     "user_tz": -330
    },
    "id": "DCj-TOrYSLV0",
    "outputId": "9503907d-c5a2-4f8f-afdd-59f3a2efe0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    5287.000000\n",
      "mean       20.258180\n",
      "std        10.006057\n",
      "min         3.000000\n",
      "25%        14.000000\n",
      "50%        19.000000\n",
      "75%        23.000000\n",
      "max        61.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train['text'].apply(lambda x: len(x.split())).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1962.000000\n",
      "mean      110.431193\n",
      "std        55.119819\n",
      "min        21.000000\n",
      "25%        73.000000\n",
      "50%       106.500000\n",
      "75%       123.000000\n",
      "max       288.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test['text'].apply(lambda x: len(x)).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VamCERzMSOQU"
   },
   "outputs": [],
   "source": [
    "train1=train.drop(['ID'],axis=1)\n",
    "test1=test.drop(['ID'],axis=1)\n",
    "test1['label']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1AhJlYnOuWNH"
   },
   "source": [
    "# Roberta Large Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "err=[]\n",
    "y_pred_tot=[]\n",
    "\n",
    "fold=StratifiedKFold(n_splits=20, shuffle=True, random_state=2)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(train1,train1['target']):\n",
    "    train1_trn, train1_val = train1.iloc[train_index], train1.iloc[test_index]\n",
    "    model = ClassificationModel('roberta', 'roberta-large', use_cuda=True,num_labels=2, args={'train_batch_size':32,\n",
    "                                                                         'reprocess_input_data': True,\n",
    "                                                                         'overwrite_output_dir': True,\n",
    "                                                                         'fp16': False,\n",
    "                                                                         'do_lower_case': False,\n",
    "                                                                         'num_train_epochs': 2,\n",
    "                                                                         'max_seq_length': 64,\n",
    "                                                                         'regression': False,\n",
    "                                                                         'manual_seed': 2,\n",
    "                                                                         \"learning_rate\":3e-5,\n",
    "                                                                         'weight_decay':0,\n",
    "                                                                         \"save_eval_checkpoints\": False,\n",
    "                                                                         \"save_model_every_epoch\": False,\n",
    "                                                                         \"silent\": True})\n",
    "    model.train_model(train1_trn)\n",
    "    raw_outputs_val = model.eval_model(train1_val)[1]\n",
    "    raw_outputs_val = softmax(raw_outputs_val,axis=1)[:,1]\n",
    "    print(f\"Log_Loss: {log_loss(train1_val['target'], raw_outputs_val)}\")\n",
    "    err.append(log_loss(train1_val['target'], raw_outputs_val))\n",
    "    raw_outputs_test = model.eval_model(test1)[1]\n",
    "    raw_outputs_test = softmax(raw_outputs_test,axis=1)[:,1]\n",
    "    y_pred_tot.append(raw_outputs_test)\n",
    "print(\"Mean LogLoss: \",np.mean(err))\n",
    "final=pd.DataFrame()\n",
    "final['ID']=test['ID']\n",
    "final['target']=np.mean(y_pred_tot, 0)\n",
    "print(final.shape)\n",
    "final.to_csv('20fold_rbl_2_3e5_32_64_0.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"20fold_rbl_2_3e5_32_64_0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Mean LogLoss: 0.2077\n",
    "#### Public lb: 0.1724\n",
    "#### Private lb: 0.1638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Large Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "err=[]\n",
    "y_pred_tot=[]\n",
    "\n",
    "fold=StratifiedKFold(n_splits=20, shuffle=True, random_state=2)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(train1,train1['target']):\n",
    "    train1_trn, train1_val = train1.iloc[train_index], train1.iloc[test_index]\n",
    "    model = ClassificationModel('roberta', 'roberta-large', use_cuda=True,num_labels=2, args={'train_batch_size':32,\n",
    "                                                                         'reprocess_input_data': True,\n",
    "                                                                         'overwrite_output_dir': True,\n",
    "                                                                         'fp16': False,\n",
    "                                                                         'do_lower_case': False,\n",
    "                                                                         'num_train_epochs': 2,\n",
    "                                                                         'max_seq_length': 64,\n",
    "                                                                         'regression': False,\n",
    "                                                                         'manual_seed': 2,\n",
    "                                                                         \"learning_rate\":5e-5,\n",
    "                                                                         'weight_decay':0,\n",
    "                                                                         \"save_eval_checkpoints\": False,\n",
    "                                                                         \"save_model_every_epoch\": False,\n",
    "                                                                         \"silent\": True})\n",
    "    model.train_model(train1_trn)\n",
    "    raw_outputs_val = model.eval_model(train1_val)[1]\n",
    "    raw_outputs_val = softmax(raw_outputs_val,axis=1)[:,1]\n",
    "    print(f\"Log_Loss: {log_loss(train1_val['target'], raw_outputs_val)}\")\n",
    "    err.append(log_loss(train1_val['target'], raw_outputs_val))\n",
    "    raw_outputs_test = model.eval_model(test1)[1]\n",
    "    raw_outputs_test = softmax(raw_outputs_test,axis=1)[:,1]\n",
    "    y_pred_tot.append(raw_outputs_test)\n",
    "print(\"Mean LogLoss: \",np.mean(err))\n",
    "final=pd.DataFrame()\n",
    "final['ID']=test['ID']\n",
    "final['target']=np.mean(y_pred_tot, 0)\n",
    "print(final.shape)\n",
    "final.to_csv('20fold_rbl_2_5e5_32_64_0.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"20fold_rbl_2_5e5_32_64_0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Mean LogLoss: 0.1974\n",
    "#### Public lb: 0.1659\n",
    "#### Private lb: 0.1660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Large Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "err=[]\n",
    "y_pred_tot=[]\n",
    "\n",
    "fold=StratifiedKFold(n_splits=20, shuffle=True, random_state=2)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(train1,train1['target']):\n",
    "    train1_trn, train1_val = train1.iloc[train_index], train1.iloc[test_index]\n",
    "    model = ClassificationModel('roberta', 'roberta-large', use_cuda=True,num_labels=2, args={'train_batch_size':32,\n",
    "                                                                         'reprocess_input_data': True,\n",
    "                                                                         'overwrite_output_dir': True,\n",
    "                                                                         'fp16': False,\n",
    "                                                                         'do_lower_case': False,\n",
    "                                                                         'num_train_epochs': 2,\n",
    "                                                                         'max_seq_length': 128,\n",
    "                                                                         'regression': False,\n",
    "                                                                         'manual_seed': 2,\n",
    "                                                                         \"learning_rate\":3e-5,\n",
    "                                                                         'weight_decay':0,\n",
    "                                                                         \"save_eval_checkpoints\": False,\n",
    "                                                                         \"save_model_every_epoch\": False,\n",
    "                                                                         \"silent\": True})\n",
    "    model.train_model(train1_trn)\n",
    "    raw_outputs_val = model.eval_model(train1_val)[1]\n",
    "    raw_outputs_val = softmax(raw_outputs_val,axis=1)[:,1]\n",
    "    print(f\"Log_Loss: {log_loss(train1_val['target'], raw_outputs_val)}\")\n",
    "    err.append(log_loss(train1_val['target'], raw_outputs_val))\n",
    "    raw_outputs_test = model.eval_model(test1)[1]\n",
    "    raw_outputs_test = softmax(raw_outputs_test,axis=1)[:,1]\n",
    "    y_pred_tot.append(raw_outputs_test)\n",
    "print(\"Mean LogLoss: \",np.mean(err))\n",
    "final=pd.DataFrame()\n",
    "final['ID']=test['ID']\n",
    "final['target']=np.mean(y_pred_tot, 0)\n",
    "print(final.shape)\n",
    "final.to_csv('20fold_rbl_2_3e5_32_128.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"20fold_rbl_2_3e5_32_128.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Mean LogLoss: 0.1998\n",
    "#### Public lb: 0.1692\n",
    "#### Private lb: 0.1663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Large Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 327473,
     "status": "error",
     "timestamp": 1589144714977,
     "user": {
      "displayName": "krishna priya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgWBAV-1HvbztQSHgm8HTqqGtrYwH3abQ0V59zh=s64",
      "userId": "17318029652689759205"
     },
     "user_tz": -330
    },
    "id": "NMeK7AzLuXZK",
    "outputId": "fb16779b-15f2-4340-d6b3-3aca0c17d0ec"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "err=[]\n",
    "y_pred_tot=[]\n",
    "\n",
    "fold=StratifiedKFold(n_splits=20, shuffle=True, random_state=2)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(train1,train1['target']):\n",
    "    train1_trn, train1_val = train1.iloc[train_index], train1.iloc[test_index]\n",
    "    model = ClassificationModel('bert', 'bert-large-uncased-whole-word-masking', use_cuda=True,num_labels=2, args={'train_batch_size':32,\n",
    "                                                                         'reprocess_input_data': True,\n",
    "                                                                         'overwrite_output_dir': True,\n",
    "                                                                         'fp16': False,\n",
    "                                                                         'do_lower_case': True,\n",
    "                                                                         'num_train_epochs': 2,\n",
    "                                                                         'max_seq_length': 64,\n",
    "                                                                         'regression': False,\n",
    "                                                                         'manual_seed': 2,\n",
    "                                                                         \"learning_rate\":5e-5,\n",
    "                                                                         'weight_decay':0,\n",
    "                                                                         \"save_eval_checkpoints\": False,\n",
    "                                                                         \"save_model_every_epoch\": False,\n",
    "                                                                         \"silent\": True})\n",
    "    model.train_model(train1_trn)\n",
    "    raw_outputs_val = model.eval_model(train1_val)[1]\n",
    "    raw_outputs_val = softmax(raw_outputs_val,axis=1)[:,1]\n",
    "    print(f\"Log_Loss: {log_loss(train1_val['target'], raw_outputs_val)}\")\n",
    "    err.append(log_loss(train1_val['target'], raw_outputs_val))\n",
    "    raw_outputs_test = model.eval_model(test1)[1]\n",
    "    raw_outputs_test = softmax(raw_outputs_test,axis=1)[:,1]\n",
    "    y_pred_tot.append(raw_outputs_test)\n",
    "print(\"Mean LogLoss: \",np.mean(err))\n",
    "final=pd.DataFrame()\n",
    "final['ID']=test['ID']\n",
    "final['target']=np.mean(y_pred_tot, 0)\n",
    "print(final.shape)\n",
    "final.to_csv('20fold_bluwwm_2_5e5_32_64.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmf6xXpQUkeo"
   },
   "outputs": [],
   "source": [
    "files.download(\"20fold_bluwwm_2_5e5_32_64.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6glELii72is"
   },
   "source": [
    "#### Local Mean LogLoss: 0.21xx\n",
    "#### Public lb: 0.1893\n",
    "#### Private lb: 0.1821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So Among these Models only Model 1 was enough to get Private Rank 1 on leaderboard. But I did not know this at that time :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here comes blending: I tried many, will show you a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr3=pd.read_csv('20fold_rbl_2_3e5_32_64_0.csv')  # Mean LogLoss: 0.2077, Public lb: 0.1724\n",
    "tr4=pd.read_csv('20fold_rbl_2_5e5_32_64_0.csv')  # Mean LogLoss: 0.1974, Public lb: 0.1659\n",
    "tr5=pd.read_csv('20fold_rbl_2_3e5_32_128.csv')   # Mean LogLoss: 0.1998, Public lb: 0.1692\n",
    "tr6=pd.read_csv('20fold_bluwwm_2_5e5_32_64.csv') # Mean LogLoss: 0.21XX, Public lb: 0.1893\n",
    "\n",
    "final=pd.DataFrame()\n",
    "final['ID'] = tr4['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['target'] =  tr4['target']*0.4 + tr5['target']*0.3 + tr3['target']*0.2 + tr6['target']*0.1\n",
    "final.to_csv('ensemble6.csv', index=False) # Private lb: 0.1597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['target'] =  tr4['target']*0.4 + tr5['target']*0.25 + tr3['target']*0.2 + tr6['target']*0.15\n",
    "final.to_csv('ensemble7.csv', index=False) # Private lb: 0.1591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['target'] =  ((tr4['target']*0.7 + tr5['target']*0.3)*0.7 + tr3['target']*0.3)*0.7 + tr6['target']*0.3\n",
    "final.to_csv('ensemble8.csv', index=False) # Private lb: 0.1588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['target'] =  tr4['target']*0.5 + tr5['target']*0.3 + tr6['target']*0.2\n",
    "final.to_csv('ensemble9.csv', index=False) # Private lb: 0.1595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of them gave private score under 16. Power of Blending :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS: I was exhausted by then so I did not note their public scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most of you would be using only roberta large and if you are wondering what was the best score with blending only roberta, it was 0.1629 Just a little less than model 1 single score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Score: ensemble8"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Some Learnings and Tips regarding using transformers:\n",
    "\n",
    "1) Find the best Learning Rate for your data.\n",
    "2) Dont frustrate over randomness of neural architectures, different random state and seed will give different results.\n",
    "3) To tackle this choose 1 random seed for a competition and try to find the minima with it. Finally Kfold is the saviour.\n",
    "4) Find the trade off between maximum sequence length and batch size according to the GPU you are using. There is no hard and fast rule for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Transformers is a good library to start using SOTA for all the noobs out there like me :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOFI8JKp6DXOfN7F+em26LI",
   "collapsed_sections": [],
   "mount_file_id": "15vQHq7CUoI7m8P4vXtIQPbqq0zK7rZD1",
   "name": "zindi_weekend.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
