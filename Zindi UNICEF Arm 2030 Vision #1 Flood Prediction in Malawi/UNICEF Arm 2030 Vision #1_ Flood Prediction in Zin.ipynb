{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom matplotlib import pyplot as plt\n\n# The following lines adjust the granularity of reporting.\npd.options.display.max_rows = 10\npd.options.display.float_format = \"{:.1f}\".format\n# tf.keras.backend.set_floatx('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/Train.csv')\ntrain.columns = train.columns.str.replace(' ', '')\ncorr_features = train[['X', 'Y', 'target_2015', 'elevation']].copy()\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = corr_features.hist(figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_features.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,6))\ncorr= corr_features.corr()\nheatmap = sns.heatmap(round(corr, 2), annot=True, ax=ax, cmap='coolwarm',fmt='.2f',linewidths=.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross 'X' and 'Y'\ntrain['XY'] = train['X']*train['Y']\ntrain[\"XY_elevation\"] = train['XY'] * train['elevation']\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot\nplt.scatter(train['XY'], train['target_2015'], alpha=0.4, edgecolors='w')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['total_precip_2015'] = 0\ntrain['total_precip_2019'] = 0\ncount_2015=0\ncount_2019=0\nfor col in train.columns:\n    if len(col) == 27:\n        if col[9] == \"5\":\n            count_2015 += 1\n            train['total_precip_2015'] += train[col]\n        elif  col[9] == \"9\":\n            count_2019 += 1\n            train['total_precip_2019'] += train[col]\n        else:\n            continue\n    else:\n        continue\ntrain[\"ave_precip_2015\"] = train['total_precip_2015'] / count_2015\ntrain[\"ave_precip_2019\"] = train['total_precip_2019'] / count_2019\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop precip columns\n# since we have computed total and average rainfall\nfor col in train.columns:\n    if len(col) == 27:\n        train = train.drop(columns=[col])\n    else:\n        continue\ntest = train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Heat mat\ncols = ['X', 'Y', 'XY', 'elevation', 'LC_Type1_mode', 'total_precip_2015', 'ave_precip_2015', 'target_2015']\nf, ax = plt.subplots(figsize=(15,10))\ncorr= corr_features.corr()\nheatmap = sns.heatmap(round(train[cols], 2), annot=False, ax=ax, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits import mplot3d\n\nfig = plt.figure(figsize=(20, 15))\nax = fig.add_subplot(111, projection='3d')\n\nxs = train['X']\nys = train['Y']\nzs = train['target_2015']\nax.scatter(xs, ys, zs, s=50, alpha=0.6, edgecolors='w')\n\nax.set_xlabel('latitude')\nax.set_ylabel('longitude')\nax.set_zlabel('target_2015')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches(15, 15)\n\nplt.scatter(x = train['X'], \n            y = train['Y'], \n            s = train['target_2015']*1000, # <== ðŸ˜€ Look here!\n            alpha=0.4, \n            edgecolors='w')\n\nplt.xlabel('latitude')\nplt.ylabel('longitude')\nplt.title('Longitude - Latitude - Target_2015', y=1.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\n\nfig = plt.gcf()\nfig.set_size_inches(50, 50)\n\nfig = px.scatter_3d(train, x='X', y='Y', z='elevation', color='target_2015', size_max=2)\nfig.update_layout(title='Elevation', autosize=True,\n                      width=500, height=500,\n                      margin=dict(l=65, r=50, b=65, t=90))\nfig.show()\n\n# fig = go.Figure(data=[go.Scatter3d(x=train['X'], y=train['Y'], z=train['elevation'], color=train['target_2015'],\n#                                    mode='markers')])\n# fig.show()\n\n# fig = go.Figure(data=[go.Surface(x=train['X'], y=train['Y'], z=train['elevation'],  surfacecolor=train['target_2015'])])\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = train.copy()\n\n# Drop precip columns from train and test\nfor col in train.columns:\n    if len(col) == 27:\n        if col[9] == \"5\":\n            train = train.drop(columns=[col])\n        elif  col[9] == \"9\":\n            test = test.drop(columns=[col])\n        else:\n            continue\n    else:\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_unpivoted = train.melt(id_vars=['Square_ID'], value_vars=['total_precip_2015', 'total_precip_2019'], var_name='year', value_name='total_precip')\ntrain_unpivoted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_unpivoted.iloc[:16466]\ntest_df = train_unpivoted.iloc[16466:]\ntrain_df = train_df.set_index('Square_ID').join(train.set_index('Square_ID')).reset_index()\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.set_index('Square_ID').join(train.set_index('Square_ID')).reset_index()\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_df.copy()\ntest = test_df.drop(columns=['target_2015'])\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target_2015'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loc = np.append(train['year'].values, test['year'].values, axis=0)\nfrom sklearn.preprocessing import LabelEncoder\nl=LabelEncoder()\nl.fit(list(set(loc)))\ntrain['year']=l.transform(train['year'])\ntest['year']=l.transform(test['year'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"o=test['Square_ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train['Square_ID']\ndel test['Square_ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=train\ntest_df=test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.drop(labels=['target_2015'], axis=1)\ny = train_df['target_2015'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_cv.shape, y_cv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt \nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_cv, label=y_cv)\n\nparam = {'objective': 'regression',\n         'num_leaves':500,\n         'boosting': 'gbdt',  \n         'metric': 'mae',\n         'learning_rate': 0.1,\n         'num_iterations': 1000,\n         'num_leaves': 80,\n         'max_depth': 8,\n         'min_data_in_leaf': 11,\n         'bagging_fraction': 0.90,\n         'bagging_freq': 1,\n         'bagging_seed': 101,\n         'feature_fraction': 0.90,\n         'feature_fraction_seed': 2,\n         'max_bin': 250\n         }\n\nlgbm = lgb.train(params=param, verbose_eval=100, train_set=train_data, valid_sets=[test_data])\n\ny_pred_lgbm = lgbm.predict(X_cv)\nprint('RMSLE:', sqrt(mean_squared_error(np.expm1(y_cv), np.expm1(y_pred_lgbm))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfeature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importance(), X.columns), reverse=True)[:50], \n                           columns=['Value','Feature'])\nplt.figure(figsize=(12, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\n\nerrlgb = []\ny_pred_totlgb = []\n\nfold = KFold(n_splits=\n             4, shuffle=True, random_state=101)\n\nfor train_index, test_index in fold.split(X):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    lgbm = LGBMRegressor(boosting_type='gbdt', num_leaves=100, max_depth=31, learning_rate=0.01, n_estimators=1000, min_child_samples=20, subsample=0.9, bagging_fraction=0.90, feature_fraction=0.90, bagging_freq=1,bagging_seed=101)\n    lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=100)\n\n    y_pred_lgbm = lgbm.predict(X_test)\n    print(\"RMSLE LGBM: \", sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_lgbm))))\n\n    errlgb.append(sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_lgbm))))\n    p = lgbm.predict(Xtest)\n    y_pred_totlgb.append(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(errlgb,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_final = np.mean(y_pred_totlgb,0)\nlgbm_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Square_ID\": o,\n        \"target_2019\": lgbm_final\n    })\nsubmission.to_csv('./submission2.csv', index=False)\nprint(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(verbose=1, learning_rate=0.1, n_estimators=500, random_state=101, subsample=0.8, loss='ls')\ngb.fit(X_train, y_train)\ny_pred = gb.predict(X_cv)\nprint('score', sqrt(mean_squared_error(y_cv, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfeature_imp = pd.DataFrame(sorted(zip(gb.feature_importances_, X.columns), reverse=True)[:60], columns=['Value','Feature'])\nplt.figure(figsize=(12,10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Gradient Boosting Features')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nerrgb = []\ny_pred_totgb = []\n\nfold = KFold(n_splits=4, shuffle=True, random_state=101)\n\nfor train_index, test_index in fold.split(X):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n             \n    #gb = GradientBoostingRegressor(learning_rate=0.9, n_estimators=100, random_state=101, subsample=0.8, loss='ls')\n    #gb = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=1000, subsample=0.8, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n    gb = GradientBoostingRegressor(learning_rate=0.1, n_estimators=1500, random_state=101, subsample=0.8, loss='ls')\n    gb.fit(X_train, y_train)\n    y_pred = gb.predict(X_test)\n    print('Score', sqrt(mean_squared_error(y_test, y_pred)))\n\n    errgb.append(sqrt(mean_squared_error(y_test, y_pred)))\n    p = gb.predict(Xtest)\n    y_pred_totgb.append(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(errgb,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = np.mean(y_pred_totgb,0)\ngb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Square_ID\": o,\n        \"target_2019\": gb\n    })\nsubmission.to_csv('./submission5.csv', index=False)\nprint(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_feature(*args):\n    for arg in args:\n        feature_name = 'norm_'+arg\n        train[feature_name] = (train[arg].copy() - train[arg].mean())/train[arg].std()\n#        test[feature_name] = (test[arg].copy() - test[arg].mean())/test[arg].std()\n        \n #       test['feature_norm'] = (test[test_feature] - test[test_feature].mean())/test[test_feature].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an empty list that will eventually hold all created feature columns.\nfeature_columns = []\n\n# Invoke the normalize feature method\nnormalize_feature(\"elevation\", \"XY\")\n\n# Create a numerical feature column to represent feature 1.\nfeature_1 = tf.feature_column.numeric_column(\"norm_elevation\")\nfeature_columns.append(feature_1)\n\n# Create a numerical feature column to represent feature 2.\nfeature_2 = tf.feature_column.numeric_column(\"norm_XY\")\nfeature_columns.append(feature_2)\n\n# Create a numerical feature column to represent longitude.\n# train[\"XY_elevation\"] = train['XY'] * train['elevation']\n# test[\"XY_elevation\"] = test['XY'] * test['elevation']\nnormalize_feature(\"XY_elevation\")\nfeature_3 = tf.feature_column.numeric_column(\"norm_XY_elevation\")\nfeature_columns.append(feature_3)\n\n# Create a numerical feature column to represent longitude.\nnormalize_feature(\"total_precip\")\nfeature_4 = tf.feature_column.numeric_column(\"norm_total_precip\")\nfeature_columns.append(feature_4)\n\n# Create a numerical feature column to represent longitude.\nnormalize_feature(\"LC_Type1_mode\")\nfeature_5 = tf.feature_column.numeric_column(\"norm_LC_Type1_mode\")\nfeature_columns.append(feature_5)\n\nprint(feature_columns)\n\n# Convert the list of feature columns into a layer that will later be fed into\n# the model. \nfeature_layer = layers.DenseFeatures(feature_columns)\n\n# Print the first 3 and last 3 rows of the feature_layer's output when applied\n# to train_df_norm:\nfeature_layer(dict(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#@title Define the functions that build and train a model\ndef build_model(my_learning_rate):\n  \"\"\"Create and compile a simple linear regression model.\"\"\"\n  # Most simple tf.keras models are sequential.\n  model = tf.keras.models.Sequential()\n  model.add(feature_layer)\n  model.add(tf.keras.layers.Dense(units=1,\n                                  input_shape=(1,),\n                                  activation='relu',\n                                  #kernel_regularizer=tf.keras.regularizers.l1(l=0.00001),\n                                  kernel_regularizer=tf.keras.regularizers.l2(l=0.00001)\n                                 ))\n\n  # Compile the model topography into code that TensorFlow can efficiently\n  # execute. Configure training to minimize the model's mean squared error. \n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n  return model        \n\n\ndef train_model(model, dataset, label_name, epochs, batch_size, shuffle=True, my_validation_split=0.2):\n  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n\n  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n  # each array contains the data for one feature.  Here, we're passing\n  # every column in the dataset. Note that the feature_layer will filter\n  # away most of those columns, leaving only the desired columns and their\n  # representations as features.\n  features = {name:np.array(value) for name, value in dataset.items()}\n  label = np.array(features.pop(label_name)) \n  history = model.fit(x=features, y=label, batch_size=batch_size,\n                      epochs=epochs, shuffle=shuffle)\n\n#   history = model.fit(x=features, y=label, batch_size=batch_size,\n#                       epochs=epochs, shuffle=shuffle,\n#                       validation_split=my_validation_split)\n\n  # Gather the trained model's weight and bias.\n  print(model.get_weights())\n  trained_weight = model.get_weights()[0]\n  trained_bias = model.get_weights()[1]\n\n  # The list of epochs is stored separately from the rest of history.\n  epochs = history.epoch\n  \n  # Isolate the error for each epoch.\n  hist = pd.DataFrame(history.history)\n\n  # To track the progression of training, we're going to take a snapshot\n  # of the model's root mean squared error at each epoch. \n  rmse = hist[\"root_mean_squared_error\"]\n\n  return trained_weight, trained_bias, epochs, rmse\n\nprint(\"Defined the create_model and traing_model functions.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#@title Define the plotting functions\ndef plot_the_model(trained_weight, trained_bias, feature, label):\n  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n\n  # Label the axes.\n  plt.xlabel(feature)\n  plt.ylabel(label)\n\n  # Create a scatter plot from 200 random points of the dataset.\n  random_examples = train.sample(n=300)\n  plt.scatter(random_examples[feature], random_examples[label])\n\n  # Create a red line representing the model. The red line starts\n  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n  x0 = -2.5\n  y0 = trained_bias\n  x1 = 2.5\n  y1 = trained_bias + (trained_weight * x1)\n  plt.plot([x0, x1], [y0, y1], c='r')\n\n  # Render the scatter plot and the red line.\n  plt.show()\n    \n\n# Needs to be updated to plot the line of best fit\n#@title Define the plotting functions\ndef plot_the_model_plotly(trained_weight_1, trained_weight_2, trained_bias, feature_1, feature_2, label):\n  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n\n#   # Label the axes.\n#   plt.xlabel(feature_1)\n#   plt.xlabel(feature_2)\n#   plt.ylabel(label)\n\n\n  # Create a surface plot representing the model\n  x, y = np.linspace(0, -10, 20).reshape(4, 5), np.linspace(0, -10, 20).reshape(4, 5)\n  z = trained_bias + (trained_weight_1 * x) + (trained_weight_2 * y)\n  fig = go.Figure(data=[go.Surface(z=z), go.Scatter3d(x=train[feature_1], y=train[feature_2], z=train[label],\n                                   mode='markers')])\n  fig.update_layout(title='Fit and scatter', autosize=True,\n                      width=500, height=500,\n                      margin=dict(l=65, r=50, b=65, t=90))\n\n#   # Create a scatter plot from 200 random points of the dataset.\n#   random_examples = train.sample(n=200)\n#   fig1 = px.scatter_3d(random_examples, x=feature_1, y=feature_2, z=label, size_max=18)\n    \n#   # Create a red line representing the model. The red line starts\n#   # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n#   x0 = -16.8\n#   y0 = trained_bias\n#   x1 = -15.2\n#   x2 = 35.5\n#   y1 = trained_bias + (trained_weight_1 * x1) + (trained_weight_2 * x2)\n#   plt.plot([x0, x1], [y0, y1], c='r')\n\n  # Render the scatter plot and the red line.\n  fig.show()\n\n\ndef plot_the_loss_curve(epochs, rmse):\n  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Root Mean Squared Error\")\n\n  plt.plot(epochs, rmse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([rmse.min()*0.95, rmse.max()*1.03])\n#   plt.ylim(0, 0.5)\n  plt.show()  \n\nprint(\"Defined the plot_the_model and plot_the_loss_curve functions.\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# The following variables are the hyperparameters.\nlearning_rate = 0.01 # originally 0.01\nepochs = 20000\nbatch_size = 10000\nvalidation_split = 0.20\n\n# # Invoke the normalize feature method\n# normalize_feature(\"ave_precip_2015\", \"ave_precip_2019\")\n\n# Specify the feature and the label.\n# my_feature = \"feature_norm\"  # the total number of rooms on a specific city block.\n\nmy_label=\"target_2015\" # the median value of a house on a specific city block.\n# That is, you're going to create a model that predicts house value based \n# solely on total_rooms.  \n\n# Discard any pre-existing version of the model.\nmy_model = None\n\n# Invoke the functions.\nmy_model = build_model(learning_rate)\n# weight, bias, epochs, rmse = train_model(my_model, train, \n#                                          my_feature, my_label,\n#                                          epochs, batch_size)\nweight, bias, epochs, rmse = train_model(my_model, train, my_label,\n                                         epochs, batch_size)\n\n# weight, bias, epochs, rmse = train_model(my_model, train, my_label,\n#                                          epochs, batch_size, validation_split)\n\n\nprint(\"\\nThe w1 learned weight for your model is %.4f\" % weight[0])\nprint(\"\\nThe w2 learned weight for your model is %.4f\" % weight[1])\nprint(\"\\nAll weights for your model are: \"+ str(weight))\nprint(\"The learned bias for your model is %.4f\\n\" % bias )\n\n# plot_the_model_plot(weight, bias, my_feature, my_label)\nplot_the_model_plotly(weight[0], weight[1], bias, \"norm_elevation\", \"norm_XY\", my_label)\nplot_the_loss_curve(epochs, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_target_2015(n, my_feature, my_label):\n  \"\"\"Predict the extent of flooding based on a feature.\"\"\"\n\n  batch = validate[my_feature].sample(n=300, random_state=1)\n  predicted_values = my_model.predict_on_batch(x=batch)\n\n  features = {name:np.array(value) for name, value in test.items()}\n  label = np.array(features.pop(my_label))\n\n  my_model.evaluate(x = features, y = label, batch_size=batch_size)\n\n  print(\"feature   label          predicted          variance\")\n  print(\"  value   value          value              value\")\n  print(\"--------------------------------------\")\n  for i in range(n):\n    print (\"%5.0f %6.0f %15.0f %15.0f \" % (train[my_feature][i],\n                                   train[label][i],\n                                   predicted_values[i][0],\n                                   train[label][i]-predicted_values[i][0]))\n    \n \n# Needs to be reworked\ndef predict_target_2019(n, feature, label):\n  \"\"\"Predict the extent of flooding based on a feature.\"\"\"\n\n  # Test using the test set\n  test = train.drop(columns=['target_2015'])\n  print(\"\\n: Evaluate the new model against the test set:\")\n  test_features = {name:np.array(value) for name, value in test.items()}\n\n  batch = test_features\n  predicted_values = my_model.predict_on_batch(x=batch)\n\n#   print(\"feature   label\")\n#   print(\"  value   value\")\n#   print(\"--------------------------------------\")\n#   for i in range(n):\n#     print (\"%5.0f %6.0f\" % (test['Suquare_ID'][i],\n#                                    predicted_values[i][0]))\n  np.savetxt(\"Akashtop1.csv\", predicted_values, delimiter=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Invoke the target prediction on 10 examples:\npredict_target_2019(16466, 'my_feature', my_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"env","language":"python","name":"env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}