{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](inn.png)\n",
    "\n",
    "# Innoplexus Online Hiring Hackathon: Sentiment Analysis\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "### Sentiment Analysis for drugs/medicines\n",
    "Nowadays the narrative of a brand is not only built and controlled by the company that owns the brand. For this reason, companies are constantly looking out across Blogs, Forums, and other social media platforms, etc for checking the sentiment for their various products and also competitor products to learn how their brand resonates in the market. This kind of analysis helps them as part of their post-launch market research. This is relevant for a lot of industries including pharma and their drugs.\n",
    " \n",
    "\n",
    "**The challenge is that the language used in this type of content is not strictly grammatically correct. Some use sarcasm. Others cover several topics with different sentiments in one post. Other users post comments and reply and thereby indicating his/her sentiment around the topic.**\n",
    "\n",
    "Sentiment can be clubbed into 3 major buckets - **Positive, Negative and Neutral Sentiments.**\n",
    "\n",
    "\n",
    "You are provided with data containing samples of text. This text can contain one or more drug mentions. Each row contains a unique combination of the text and the drug mention. Note that the same text can also have different sentiment for a different drug.\n",
    "\n",
    "Given the text and drug name, the task is to predict the sentiment for texts contained in the test dataset. Given below is an example of text from the dataset:\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "*Stelara is still fairly new to Crohn's treatment. This is why you might not get a lot of replies. I've done some research, but most of the \"time to work\" answers are from Psoriasis boards. For Psoriasis, it seems to be about 4-12 weeks to reach a strong therapeutic level. The good news is, Stelara seems to be getting rave reviews from Crohn's patients. It seems to be the best med to come along since Remicade. I hope you have good success with it. My daughter was diagnosed Feb. 19/07, (13 yrs. old at the time of diagnosis), with Crohn's of the Terminal Illium. Has used Prednisone and Pentasa. Started Imuran (02/09), had an abdominal abscess (12/08). 2cm of Stricture. Started â€‹Remicade in Feb. 2014, along with 100mgs. of Imuran.*\n",
    "\n",
    "\n",
    "For Stelara the above text is **positive** while for Remicade the above text is **negative**.\n",
    "\n",
    "### Data Description\n",
    "**train.csv**\n",
    "Contains the labelled texts with sentiment values for a given drug\n",
    " \n",
    "|Variable|\tDefinition|\n",
    "|----|----|\n",
    "|unique_hash |Unique ID|\n",
    "|text|text pertaining to the drugs|\n",
    "|drug |drug name for which the sentiment is provided|\n",
    "|sentiment\t|(Target) 0-positive, 1-negative, 2-neutral  |\n",
    "\n",
    "\n",
    "**test.csv**\n",
    "test.csv contains texts with drug names for which the participants are expected to predict the correct sentiment\n",
    " \n",
    "\n",
    "### Evaluation Metric\n",
    "The metric used for evaluating the performance of the classification model would be macro F1-Score.\n",
    " \n",
    "\n",
    "## Public and Private Split\n",
    "\n",
    "The texts in the test data are further randomly divided into Public (40%) and Private (60%) data.\n",
    "Your initial responses will be checked and scored on the Public data.\n",
    "The final rankings would be based on your private score which will be published once the competition is over.\n",
    "\n",
    "# Approaches\n",
    "\n",
    "\n",
    "\n",
    "# Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['av _ innoplexus hiring']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (5279, 4)\n",
      "Test shape: (2924, 3)\n",
      "Number of classes: 3\n",
      "Class distribution:\n",
      "2    0.724569\n",
      "1    0.158553\n",
      "0    0.116878\n",
      "Name: sentiment, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:100: DeprecationWarning: The ``n_values_`` attribute was deprecated in version 0.20 and will be removed 0.22.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import textblob\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "os.environ['PYTHONHASHSEED'] = '10000'\n",
    "np.random.seed(10001)\n",
    "import random\n",
    "import tensorflow as tf\n",
    "random.seed(10002)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=6, inter_op_parallelism_threads=5)\n",
    "from keras import backend\n",
    "\n",
    "tf.set_random_seed(10003)\n",
    "backend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(\"../input/av _ innoplexus hiring/train_F3WbcTw.csv\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "test = pd.read_csv(\"../input/av _ innoplexus hiring/test_tOlRoBf.csv\")\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(train[\"sentiment\"].values.reshape(-1, 1))\n",
    "print(\"Number of classes:\", enc.n_values_[0])\n",
    "\n",
    "print(\"Class distribution:\\n{}\".format(train[\"sentiment\"].value_counts()/train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db45eb314d6840613f1646cc1267093da740b436"
   },
   "source": [
    "For the examples which occur in both sets, we can directly use the labels from train set as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape\n",
    "730/2924\n",
    "len(set(train[\"text\"]+' '+train[\"drug\"]).intersection(set(test[\"text\"]+' '+test[\"drug\"])))/test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common=np.intersect1d(train.text,test.text)\n",
    "test_common=test.query('text in @common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.query('text not in @common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2999\n",
       "1     837\n",
       "0     617\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_drugs=train.drug.unique()\n",
    "# all_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['drug']=='gilenya'].text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.query('text not in @common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2081, 3), (4453, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape,train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train[\"text\"]).intersection(set(test[\"text\"])))/test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(train.text,test.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0c15a8123acb6cc1d1967e47fb15d7c4333b9370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of test set examples which occur in the train set: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(set(train[\"text\"]).intersection(set(test[\"text\"])))/test.shape[0]))\n",
    "# test = pd.merge(test, train[[\"text\", \"sentiment\"]], on=\"text\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why youâ€™d want to ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                                text        drug  sentiment  \n",
       "0  Autoimmune diseases tend to come in clusters. ...     gilenya          2  \n",
       "1  I can completely understand why youâ€™d want to ...     gilenya          2  \n",
       "2  Interesting that it only targets S1P-1/5 recep...  fingolimod          2  \n",
       "3  Very interesting, grand merci. Now I wonder wh...     ocrevus          2  \n",
       "4  Hi everybody, My latest MRI results for Brain ...     gilenya          1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f23154aa33cb61e925d0c9fb557915ceafc226c5"
   },
   "source": [
    "Let's see if all the words in the test set occurs in the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "04bea1b921fb53f77ee8b069d864d2c103fec0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Vocabulary Size: 36569\n",
      "Test Set Vocabulary Size: 25257\n",
      "Number of Words that occur in both: 17558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv1 = CountVectorizer()\n",
    "cv1.fit(train[\"text\"])\n",
    "\n",
    "cv2 = CountVectorizer()\n",
    "cv2.fit(test[\"text\"])\n",
    "\n",
    "print(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\n",
    "print(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\n",
    "print(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d0cefa184547a5e02f5f389216e1a496f90e982"
   },
   "source": [
    "## Data Augmentation for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5669, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hope that it does work out, I really do. I c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There so much still to do before this is convi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm very pleased that something is being devel...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  Autoimmune diseases tend to come in clusters. ...          2\n",
       "1  I hope that it does work out, I really do. I c...          2\n",
       "2  There so much still to do before this is convi...          2\n",
       "3  Interesting that it only targets S1P-1/5 recep...          2\n",
       "4  I'm very pleased that something is being devel...          2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import json\n",
    "random.seed(1994)\n",
    "def tokenize(text):\n",
    "    '''text: list of text documents'''\n",
    "    tokenized =  sent_tokenize(text)\n",
    "    return tokenized\n",
    "\n",
    "def shuffle_tokenized(text):\n",
    "    random.shuffle(text)\n",
    "    newl=list(text)\n",
    "    shuffled.append(newl)\n",
    "    return text\n",
    "\n",
    "\n",
    "df_train=train[['text','sentiment']]\n",
    "\n",
    "augmented = []\n",
    "reps=[]\n",
    "for ng_rev in df_train[df_train.sentiment==2].text:\n",
    "    tok = tokenize(ng_rev)\n",
    "    shuffled= [tok]\n",
    "    #print(ng_rev)\n",
    "    for i in range(2):\n",
    "    #generate 11 new reviews\n",
    "        shuffle_tokenized(shuffled[-1])\n",
    "    for k in shuffled:\n",
    "        '''create new review by joining the shuffled sentences'''\n",
    "        s = ' '\n",
    "        new_rev = s.join(k)\n",
    "        if new_rev not in augmented:\n",
    "            augmented.append(new_rev)\n",
    "        else:\n",
    "            reps.append(new_rev)\n",
    "df2=pd.DataFrame({'text':augmented,'sentiment':[2]*len(augmented)})\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5266, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This could represent artifact or early axonal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the kind of symptoms from C2-C3 lesio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the kind of symptoms from C2-C3 lesio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the kind of symptoms from C2-C3 lesio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This could represent artifact or early axonal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  This could represent artifact or early axonal ...          1\n",
       "1  What are the kind of symptoms from C2-C3 lesio...          1\n",
       "2  What are the kind of symptoms from C2-C3 lesio...          1\n",
       "3  What are the kind of symptoms from C2-C3 lesio...          1\n",
       "4  This could represent artifact or early axonal ...          1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "augmented = []\n",
    "reps=[]\n",
    "for ng_rev in df_train[df_train.sentiment==1].text:\n",
    "    tok = tokenize(ng_rev)\n",
    "    shuffled= [tok]\n",
    "    #print(ng_rev)\n",
    "    for i in range(7):\n",
    "    #generate 11 new reviews\n",
    "        shuffle_tokenized(shuffled[-1])\n",
    "    for k in shuffled:\n",
    "        '''create new review by joining the shuffled sentences'''\n",
    "        s = ' '\n",
    "        new_rev = s.join(k)\n",
    "        if new_rev not in augmented:\n",
    "            augmented.append(new_rev)\n",
    "        else:\n",
    "            reps.append(new_rev)\n",
    "df1=pd.DataFrame({'text':augmented,'sentiment':[1]*len(augmented)})\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4713, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you would like to talk, contact the Help Ce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To learn more view our Understanding IBD Medic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Humira and other biologics are very successful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First, I know you said that you are scared of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can learn more about some of your treatmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  If you would like to talk, contact the Help Ce...          0\n",
       "1  To learn more view our Understanding IBD Medic...          0\n",
       "2  Humira and other biologics are very successful...          0\n",
       "3  First, I know you said that you are scared of ...          0\n",
       "4  You can learn more about some of your treatmen...          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "augmented = []\n",
    "reps=[]\n",
    "for ng_rev in df_train[df_train.sentiment==0].text:\n",
    "    tok = tokenize(ng_rev)\n",
    "    shuffled= [tok]\n",
    "    #print(ng_rev)\n",
    "    for i in range(9):\n",
    "    #generate 11 new reviews\n",
    "        shuffle_tokenized(shuffled[-1])\n",
    "    for k in shuffled:\n",
    "        '''create new review by joining the shuffled sentences'''\n",
    "        s = ' '\n",
    "        new_rev = s.join(k)\n",
    "        if new_rev not in augmented:\n",
    "            augmented.append(new_rev)\n",
    "        else:\n",
    "            reps.append(new_rev)\n",
    "df0=pd.DataFrame({'text':augmented,'sentiment':[0]*len(augmented)})\n",
    "print(df0.shape)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4713, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15648, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0=df0.append(df1)\n",
    "df0=df0.append(df2)\n",
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reps\n",
    "\n",
    "# df2=augment_text(df_train[df_train.sentiment==2].text,2,2)\n",
    "# df1=augment_text(df_train[df_train.sentiment==1].text,7,1)\n",
    "# df0=augment_text(df_train[df_train.sentiment==0].text,9,0)\n",
    "# print(df0.shape,df1.shape,df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you would like to talk, contact the Help Ce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To learn more view our Understanding IBD Medic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Humira and other biologics are very successful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First, I know you said that you are scared of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can learn more about some of your treatmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  If you would like to talk, contact the Help Ce...          0\n",
       "1  To learn more view our Understanding IBD Medic...          0\n",
       "2  Humira and other biologics are very successful...          0\n",
       "3  First, I know you said that you are scared of ...          0\n",
       "4  You can learn more about some of your treatmen...          0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5669, 2), (4713, 2), (5266, 2))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.sentiment==2].shape,train[train.sentiment==0].shape,train[train.sentiment==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "2    0.362283\n",
      "1    0.336529\n",
      "0    0.301189\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution:\\n{}\".format(train[\"sentiment\"].value_counts()/train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['If you would like to talk, contact the Help Center at 888-694-8872 or at info@crohnscolitisfoundation.org To reduce your level of fear it can help to learn more about your treatment option. Hi Jess Sorry to read about the challenges you are having with your health. You mentioned a lot in your post. I just want to share some info on a few of the points. To learn more view our Understanding IBD Medication brochure at: http://www.crohnscolitisfoundation.org/assets/pdfs/understanding-ibd-meds-nov.pdf . Reply posted for JessZidek. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. First, I know you said that you are scared of Humira. You can learn more about some of your treatment options.',\n",
       "       'To learn more view our Understanding IBD Medication brochure at: http://www.crohnscolitisfoundation.org/assets/pdfs/understanding-ibd-meds-nov.pdf . You can learn more about some of your treatment options. You mentioned a lot in your post. First, I know you said that you are scared of Humira. Hi Jess Sorry to read about the challenges you are having with your health. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. To reduce your level of fear it can help to learn more about your treatment option. Reply posted for JessZidek. I just want to share some info on a few of the points. If you would like to talk, contact the Help Center at 888-694-8872 or at info@crohnscolitisfoundation.org',\n",
       "       'Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. First, I know you said that you are scared of Humira. If you would like to talk, contact the Help Center at 888-694-8872 or at info@crohnscolitisfoundation.org To reduce your level of fear it can help to learn more about your treatment option. Reply posted for JessZidek. To learn more view our Understanding IBD Medication brochure at: http://www.crohnscolitisfoundation.org/assets/pdfs/understanding-ibd-meds-nov.pdf . Hi Jess Sorry to read about the challenges you are having with your health. You can learn more about some of your treatment options. You mentioned a lot in your post. I just want to share some info on a few of the points.',\n",
       "       ..., ' All the best to your husband and family.',\n",
       "       \"They will give you more inj as you need them, aiming to lengthen time between. Hi bazza, luckily my eyes aren't so badly affected ( I just get headaches and eye strain) so I can't help you with how to cope with the kind of difference you currently have. I had them bring my vision back from a big grey blob on more than one occasion. There is every chance the next two Lucentis inj will do the trick. I'm in year 3 of treatment. Best of luck going forward, this is a great support forum, let us know how you get on. I do however have macular oedema following branch retinal vein occlusion. Don't despair yet.\",\n",
       "       \"They will give you more inj as you need them, aiming to lengthen time between. There is every chance the next two Lucentis inj will do the trick. Don't despair yet. Best of luck going forward, this is a great support forum, let us know how you get on. I had them bring my vision back from a big grey blob on more than one occasion. I'm in year 3 of treatment. I do however have macular oedema following branch retinal vein occlusion. Hi bazza, luckily my eyes aren't so badly affected ( I just get headaches and eye strain) so I can't help you with how to cope with the kind of difference you currently have.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "2b4043b8831b8a42bd946f762375a3a97516dfc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_upper</th>\n",
       "      <th>after_comma</th>\n",
       "      <th>sentence_start</th>\n",
       "      <th>sentence_end</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>354.334394</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.924676</td>\n",
       "      <td>0.841502</td>\n",
       "      <td>2128.943560</td>\n",
       "      <td>4.821606</td>\n",
       "      <td>142.037556</td>\n",
       "      <td>4.300870</td>\n",
       "      <td>5.743732</td>\n",
       "      <td>62.471886</td>\n",
       "      <td>15.445152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272.493164</td>\n",
       "      <td>0.997152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930308</td>\n",
       "      <td>0.814280</td>\n",
       "      <td>1550.847133</td>\n",
       "      <td>4.625931</td>\n",
       "      <td>120.075389</td>\n",
       "      <td>2.698823</td>\n",
       "      <td>5.559593</td>\n",
       "      <td>43.669958</td>\n",
       "      <td>11.440562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>310.137767</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.911272</td>\n",
       "      <td>0.786382</td>\n",
       "      <td>1869.665020</td>\n",
       "      <td>4.852180</td>\n",
       "      <td>122.555830</td>\n",
       "      <td>3.619862</td>\n",
       "      <td>5.754940</td>\n",
       "      <td>58.436409</td>\n",
       "      <td>13.941259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_count  has_upper  after_comma  sentence_start  sentence_end  \\\n",
       "sentiment                                                                     \n",
       "0          354.334394   0.998939     0.000000        0.924676      0.841502   \n",
       "1          272.493164   0.997152     0.000000        0.930308      0.814280   \n",
       "2          310.137767   0.995414     0.000529        0.911272      0.786382   \n",
       "\n",
       "            char_count  avg_word   stopwords  numerics  word_density  \\\n",
       "sentiment                                                              \n",
       "0          2128.943560  4.821606  142.037556  4.300870      5.743732   \n",
       "1          1550.847133  4.625931  120.075389  2.698823      5.559593   \n",
       "2          1869.665020  4.852180  122.555830  3.619862      5.754940   \n",
       "\n",
       "           punctuation_count      upper  \n",
       "sentiment                                \n",
       "0                  62.471886  15.445152  \n",
       "1                  43.669958  11.440562  \n",
       "2                  58.436409  13.941259  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#     df[\"phrase_count\"] = df.groupby(\"drug\")[\"text\"].transform(\"count\")\n",
    "# df['drug']=pd.factorize(df['drug'])[0]\n",
    "\n",
    "def transform(df):\n",
    "#     df['drug_count']=df['text'].apply(lambda x: len(np.intersect1d(x.split(),all_drugs)))\n",
    "#     df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "    df[\"has_upper\"] = df[\"text\"].apply(lambda x: x.lower() != x)\n",
    "    df['upper'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    df[\"sentence_end\"] = df[\"text\"].apply(lambda x: x.endswith(\".\"))\n",
    "    df[\"after_comma\"] = df[\"text\"].apply(lambda x: x.startswith(\",\"))\n",
    "    df[\"sentence_start\"] = df[\"text\"].apply(lambda x: \"A\" <= x[0] <= \"Z\")\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "    import string\n",
    "    punctuation=string.punctuation\n",
    "    df['word_count']=df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['char_count'] = df['text'].str.len()\n",
    "    def avg_word(sentence):\n",
    "        words = sentence.split()\n",
    "        return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "    df['avg_word'] = df['text'].apply(lambda x: avg_word(x))\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df['stopwords'] = df['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "    df['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    \n",
    "    df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
    "    \n",
    "    return df\n",
    "\n",
    "train = transform(train)\n",
    "test = transform(test)\n",
    "\n",
    "# dense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\",'char_count','avg_word','stopwords','numerics','word_density','punctuation_count','drug','upper']\n",
    "dense_features = [ \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\",'char_count','avg_word','stopwords','numerics','word_density','punctuation_count','upper']\n",
    "train.groupby(\"sentiment\")[dense_features].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8173232d306f83f14b5a03a7940caa89d2434f17"
   },
   "source": [
    "**Splitting Data into folds**\n",
    "\n",
    "If we split the data totally random, we may bias our validation set because the phrases in the same sentence may be distributed to train and validation sets. We need to guarantee that all phrases of one sentence is in one fold. We can assume that SentenceId%NUM_FOLDS preserves this while splitting the data randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"drug\"]%5\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def url_to_words(raw_text):\n",
    "    raw_text=raw_text.strip()\n",
    "    no_coms=re.sub(r'\\.com','',raw_text)\n",
    "    no_urls=re.sub('https?://www','',no_coms)\n",
    "    no_urls1=re.sub('https?://','',no_urls)\n",
    "    try:\n",
    "        no_encoding=no_urls1.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        no_encoding = no_urls1\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \",no_encoding) \n",
    "    words = letters_only.split()                             \n",
    "    stops = stopwords.words('english')         \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n",
    "\n",
    "train['text']=train['text'].apply(url_to_words)\n",
    "test['text']=test['text'].apply(url_to_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2081, 15)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5669\n",
       "1    5266\n",
       "0    4713\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "bfaeff0ff308a70c2b254c1c6d90a97491f567c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>has_upper</th>\n",
       "      <th>upper</th>\n",
       "      <th>sentence_end</th>\n",
       "      <th>after_comma</th>\n",
       "      <th>sentence_start</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>fold_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>would like talk contact help center 888 694 88...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>118</td>\n",
       "      <td>755</td>\n",
       "      <td>5.40678</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>6.344538</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learn view understanding ibd medication brochu...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>118</td>\n",
       "      <td>755</td>\n",
       "      <td>5.40678</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>6.344538</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humira biologics successful reducing symptoms ...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>118</td>\n",
       "      <td>755</td>\n",
       "      <td>5.40678</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>6.344538</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first know said scared humira want share info ...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>118</td>\n",
       "      <td>755</td>\n",
       "      <td>5.40678</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>6.344538</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>learn treatment options humira biologics succe...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>118</td>\n",
       "      <td>755</td>\n",
       "      <td>5.40678</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>6.344538</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  has_upper  \\\n",
       "0  would like talk contact help center 888 694 88...          0       True   \n",
       "1  learn view understanding ibd medication brochu...          0       True   \n",
       "2  humira biologics successful reducing symptoms ...          0       True   \n",
       "3  first know said scared humira want share info ...          0       True   \n",
       "4  learn treatment options humira biologics succe...          0       True   \n",
       "\n",
       "   upper  sentence_end  after_comma  sentence_start  word_count  char_count  \\\n",
       "0      3          True        False            True         118         755   \n",
       "1      3         False        False            True         118         755   \n",
       "2      3          True        False            True         118         755   \n",
       "3      3         False        False            True         118         755   \n",
       "4      3          True        False            True         118         755   \n",
       "\n",
       "   avg_word  stopwords  numerics  word_density  punctuation_count  fold_id  \n",
       "0   5.40678         61         0      6.344538                 28        0  \n",
       "1   5.40678         61         0      6.344538                 28        1  \n",
       "2   5.40678         61         0      6.344538                 28        2  \n",
       "3   5.40678         61         0      6.344538                 28        3  \n",
       "4   5.40678         61         0      6.344538                 28        4  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_FOLDS = 7\n",
    "train[\"fold_id\"] = train.reset_index()['index'].apply(lambda x: x%NUM_FOLDS)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"fold_id\"].value_counts()\n",
    "# train.groupby(['fold_id','sentiment']).count()\n",
    "from sklearn.utils import shuffle\n",
    "train=shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "572a4bda2d8b1cd035f9c6fffa403e7f1b0fbbe4"
   },
   "source": [
    "**Transfer Learning Using GLOVE Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "3e18956c6b5c5c88e7097c41138a2afb3f4bc1e1"
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_FILE = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# all_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n",
    "\n",
    "# def get_embedding():\n",
    "#     embeddings_index = {}\n",
    "#     f = open(EMBEDDING_FILE)\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n",
    "#             coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "#             embeddings_index[word] = coefs\n",
    "#     f.close()\n",
    "#     return embeddings_index\n",
    "\n",
    "# embeddings_index = get_embedding()\n",
    "# print(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5669\n",
       "1    5266\n",
       "0    4713\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()\n",
    "# print(2999-837)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5266, 15)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "train[train['sentiment']==1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f93b76d710ea94ed48627babc49bc5261647789"
   },
   "source": [
    "**Prepare the sequences for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "5d2f6c292c46b367b0ae7f98f8a1fbafc163a0e0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(np.append(train[\"text\"].values, test[\"text\"].values))\n",
    "word_index = tokenizer.word_index\n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(train[\"text\"]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"text\"]), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43974"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7f3d8de4369b676ffab460533ace55ed08a10ab"
   },
   "source": [
    "**Define the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "878174c7d497cf794531afbcc8d81b5f59c803dd"
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "\n",
    "def build_model():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                300,\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    dropout = SpatialDropout1D(0.2)\n",
    "    mask_layer = Masking()\n",
    "    lstm_layer = LSTM(100,recurrent_dropout=0.2)\n",
    "    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    dense_input = Input(shape=(len(dense_features),))\n",
    "    dense_vector = BatchNormalization()(dense_input)\n",
    "    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n",
    "    \n",
    "    feature_vector = concatenate([phrase_vector, dense_vector])\n",
    "    feature_vector = Dense(64, activation=\"relu\")(feature_vector)\n",
    "#     feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n",
    "    \n",
    "    output = Dense(3, activation=\"softmax\")(feature_vector)\n",
    "    \n",
    "    model = Model(inputs=[seq_input, dense_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "def build_model_only():\n",
    "    model5_CNN= Sequential()\n",
    "    model5_CNN.add(Embedding(nb_words,300,input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model5_CNN.add(Dropout(0.2))\n",
    "    model5_CNN.add(Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1))\n",
    "    model5_CNN.add(GlobalMaxPooling1D())\n",
    "    model5_CNN.add(Dense(128,activation='relu'))\n",
    "    model5_CNN.add(Dropout(0.2))\n",
    "    model5_CNN.add(Dense(3,activation='softmax'))\n",
    "    model5_CNN.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model5_CNN\n",
    "\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "def buildAtt_layer():\n",
    "\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = Embedding(nb_words, 300)(inp)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(3, activation=\"softmax\")(x)\n",
    "    modelATT = Model(inputs=inp, outputs=x)\n",
    "#     modelATT.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
    "    modelATT.summary()\n",
    "    return modelATT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6879dab4a2dc2daa0a63ab689a7a579a4c4baa5e"
   },
   "source": [
    "**Train the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_count',\n",
       " 'has_upper',\n",
       " 'after_comma',\n",
       " 'sentence_start',\n",
       " 'sentence_end',\n",
       " 'char_count',\n",
       " 'avg_word',\n",
       " 'stopwords',\n",
       " 'numerics',\n",
       " 'word_density',\n",
       " 'punctuation_count',\n",
       " 'upper']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_upper</th>\n",
       "      <th>after_comma</th>\n",
       "      <th>sentence_start</th>\n",
       "      <th>sentence_end</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>72</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.545455</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>146</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5.214286</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>114</td>\n",
       "      <td>5.388889</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>105</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>559</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>5.273585</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>167</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>881</td>\n",
       "      <td>4.281437</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>5.244048</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  has_upper  after_comma  sentence_start  sentence_end  \\\n",
       "0          10      False        False           False         False   \n",
       "1          27       True        False            True         False   \n",
       "5          18       True        False            True         False   \n",
       "6         105       True        False            True         False   \n",
       "7         167       True        False            True          True   \n",
       "\n",
       "   char_count  avg_word  stopwords  numerics  word_density  punctuation_count  \\\n",
       "0          72  6.300000          3         1      6.545455                  4   \n",
       "1         146  4.444444         15         0      5.214286                  4   \n",
       "5         114  5.388889          7         0      6.000000                  4   \n",
       "6         559  4.333333         42         2      5.273585                 26   \n",
       "7         881  4.281437         83         5      5.244048                 19   \n",
       "\n",
       "   upper  \n",
       "0      0  \n",
       "1      1  \n",
       "5      1  \n",
       "6      8  \n",
       "7      7  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_seq\n",
    "test[dense_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13410 samples, validate on 2238 samples\n",
      "Epoch 1/10\n",
      "13410/13410 [==============================] - 20s 1ms/step - loss: 0.5015 - acc: 0.7849 - val_loss: 0.1441 - val_acc: 0.9562\n",
      "Epoch 2/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0569 - acc: 0.9857 - val_loss: 0.0881 - val_acc: 0.9812\n",
      "Epoch 3/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0342 - acc: 0.9932 - val_loss: 0.0991 - val_acc: 0.9830\n",
      "Epoch 4/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0322 - acc: 0.9938 - val_loss: 0.1029 - val_acc: 0.9821\n",
      "Epoch 5/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0244 - acc: 0.9952 - val_loss: 0.1028 - val_acc: 0.9777\n",
      "Epoch 00005: early stopping\n",
      "2238/2238 [==============================] - 0s 191us/step\n",
      "Evaluation 0.9777258536766964\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 174us/step\n",
      "\n",
      "FOLD 2\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13410 samples, validate on 2238 samples\n",
      "Epoch 1/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.4912 - acc: 0.7895 - val_loss: 0.1295 - val_acc: 0.9643\n",
      "Epoch 2/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0630 - acc: 0.9841 - val_loss: 0.0590 - val_acc: 0.9848\n",
      "Epoch 3/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0441 - acc: 0.9916 - val_loss: 0.0668 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0268 - acc: 0.9950 - val_loss: 0.0619 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0164 - acc: 0.9971 - val_loss: 0.0576 - val_acc: 0.9879\n",
      "Epoch 6/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0181 - acc: 0.9963 - val_loss: 0.0814 - val_acc: 0.9812\n",
      "Epoch 7/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0261 - acc: 0.9934 - val_loss: 0.0671 - val_acc: 0.9861\n",
      "Epoch 00007: early stopping\n",
      "2238/2238 [==============================] - 0s 210us/step\n",
      "Evaluation 0.9862046295710885\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 171us/step\n",
      "\n",
      "FOLD 3\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13413 samples, validate on 2235 samples\n",
      "Epoch 1/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.4939 - acc: 0.7876 - val_loss: 0.1063 - val_acc: 0.9660\n",
      "Epoch 2/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.0594 - acc: 0.9860 - val_loss: 0.0546 - val_acc: 0.9884\n",
      "Epoch 3/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.0365 - acc: 0.9932 - val_loss: 0.0547 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.0234 - acc: 0.9965 - val_loss: 0.0493 - val_acc: 0.9875\n",
      "Epoch 00004: early stopping\n",
      "2235/2235 [==============================] - 1s 230us/step\n",
      "Evaluation 0.9876733006228374\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 172us/step\n",
      "\n",
      "FOLD 4\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13413 samples, validate on 2235 samples\n",
      "Epoch 1/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.4916 - acc: 0.7868 - val_loss: 0.1257 - val_acc: 0.9638\n",
      "Epoch 2/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.0648 - acc: 0.9851 - val_loss: 0.0747 - val_acc: 0.9830\n",
      "Epoch 3/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.0391 - acc: 0.9934 - val_loss: 0.2028 - val_acc: 0.9472\n",
      "Epoch 4/10\n",
      "10120/13413 [=====================>........] - ETA: 3s - loss: 0.0352 - acc: 0.9941"
     ]
    }
   ],
   "source": [
    "test_preds1 = np.zeros((test.shape[0], 3))\n",
    "from sklearn.metrics import f1_score\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(\"FOLD\", i+1)\n",
    "    \n",
    "    print(\"Splitting the data into train and validation...\")\n",
    "    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n",
    "    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n",
    "    y_train = enc.transform(train[train[\"fold_id\"] != i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    y_val = enc.transform(train[train[\"fold_id\"] == i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "#     model = build_model_only()\n",
    "    model = build_model_only()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit(train_seq, y_train, validation_data=(val_seq, y_val),\n",
    "              epochs=10, batch_size=8, shuffle=True, callbacks=[early_stopping], verbose=1)\n",
    "#     print(np.argmax(model.predict([val_seq, val_dense[dense_features]], batch_size=128, verbose=1),axis=1),y_val)\n",
    "    print('Evaluation',f1_score(np.argmax(model.predict(val_seq, batch_size=8, verbose=1),axis=1),train[train[\"fold_id\"] == i][\"sentiment\"].values,average='macro'))\n",
    "    print(\"Predicting...\")\n",
    "    test_preds1 += model.predict(test_seq, batch_size=8, verbose=1)\n",
    "    print()\n",
    "    \n",
    "test_preds1 /= NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "0d4cdf9226840afc92b25b0ca75b4ff520dabf7b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13410 samples, validate on 2238 samples\n",
      "Epoch 1/10\n",
      "13410/13410 [==============================] - 20s 2ms/step - loss: 1.1220 - acc: 0.3736 - val_loss: 1.0750 - val_acc: 0.4187\n",
      "Epoch 2/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 1.0358 - acc: 0.4643 - val_loss: 0.9651 - val_acc: 0.5210\n",
      "Epoch 3/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.8224 - acc: 0.6386 - val_loss: 0.7197 - val_acc: 0.6845\n",
      "Epoch 4/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.5652 - acc: 0.7785 - val_loss: 0.4881 - val_acc: 0.7895\n",
      "Epoch 5/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.3540 - acc: 0.8657 - val_loss: 0.3751 - val_acc: 0.8610\n",
      "Epoch 6/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.1980 - acc: 0.9310 - val_loss: 0.2772 - val_acc: 0.8981\n",
      "Epoch 7/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.1177 - acc: 0.9646 - val_loss: 0.2243 - val_acc: 0.9303\n",
      "Epoch 8/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.0776 - acc: 0.9766 - val_loss: 0.2120 - val_acc: 0.9343\n",
      "Epoch 9/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.0639 - acc: 0.9815 - val_loss: 0.2629 - val_acc: 0.9138\n",
      "Epoch 10/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.0462 - acc: 0.9860 - val_loss: 0.2286 - val_acc: 0.9374\n",
      "2238/2238 [==============================] - 6s 3ms/step\n",
      "Evaluation 0.9377159973285935\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 484us/step\n",
      "\n",
      "FOLD 2\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13410 samples, validate on 2238 samples\n",
      "Epoch 1/10\n",
      "13410/13410 [==============================] - 20s 2ms/step - loss: 1.0988 - acc: 0.3807 - val_loss: 1.0607 - val_acc: 0.4285\n",
      "Epoch 2/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.9928 - acc: 0.5122 - val_loss: 0.8362 - val_acc: 0.6354\n",
      "Epoch 3/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.6892 - acc: 0.7066 - val_loss: 0.5691 - val_acc: 0.7663\n",
      "Epoch 4/10\n",
      "13410/13410 [==============================] - 19s 1ms/step - loss: 0.4531 - acc: 0.8328 - val_loss: 0.3665 - val_acc: 0.8633\n",
      "Epoch 5/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.2670 - acc: 0.9076 - val_loss: 0.2979 - val_acc: 0.8874\n",
      "Epoch 6/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.1637 - acc: 0.9459 - val_loss: 0.2379 - val_acc: 0.9142\n",
      "Epoch 7/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.1152 - acc: 0.9641 - val_loss: 0.1838 - val_acc: 0.9343\n",
      "Epoch 8/10\n",
      "13410/13410 [==============================] - 17s 1ms/step - loss: 0.0974 - acc: 0.9705 - val_loss: 0.2275 - val_acc: 0.9245\n",
      "Epoch 9/10\n",
      "13410/13410 [==============================] - 18s 1ms/step - loss: 0.0638 - acc: 0.9826 - val_loss: 0.2066 - val_acc: 0.9285\n",
      "Epoch 00009: early stopping\n",
      "2238/2238 [==============================] - 7s 3ms/step\n",
      "Evaluation 0.9291658675665698\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 482us/step\n",
      "\n",
      "FOLD 3\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13413 samples, validate on 2235 samples\n",
      "Epoch 1/10\n",
      "13413/13413 [==============================] - 21s 2ms/step - loss: 1.1168 - acc: 0.3687 - val_loss: 1.0812 - val_acc: 0.3906\n",
      "Epoch 2/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 1.0387 - acc: 0.4608 - val_loss: 0.9804 - val_acc: 0.5110\n",
      "Epoch 3/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.8166 - acc: 0.6502 - val_loss: 0.6927 - val_acc: 0.7275\n",
      "Epoch 4/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.4853 - acc: 0.8197 - val_loss: 0.4131 - val_acc: 0.8506\n",
      "Epoch 5/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.2641 - acc: 0.9067 - val_loss: 0.3276 - val_acc: 0.8796\n",
      "Epoch 6/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.1658 - acc: 0.9449 - val_loss: 0.3157 - val_acc: 0.8872\n",
      "Epoch 7/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0990 - acc: 0.9684 - val_loss: 0.2099 - val_acc: 0.9311\n",
      "Epoch 8/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0677 - acc: 0.9800 - val_loss: 0.2229 - val_acc: 0.9329\n",
      "Epoch 9/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0709 - acc: 0.9783 - val_loss: 0.3228 - val_acc: 0.8908\n",
      "Epoch 10/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0587 - acc: 0.9804 - val_loss: 0.2242 - val_acc: 0.9356\n",
      "2235/2235 [==============================] - 7s 3ms/step\n",
      "Evaluation 0.9356892220224338\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 486us/step\n",
      "\n",
      "FOLD 4\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13413 samples, validate on 2235 samples\n",
      "Epoch 1/10\n",
      "13413/13413 [==============================] - 22s 2ms/step - loss: 1.0912 - acc: 0.3855 - val_loss: 1.0629 - val_acc: 0.4349\n",
      "Epoch 2/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.9888 - acc: 0.5125 - val_loss: 0.8513 - val_acc: 0.6219\n",
      "Epoch 3/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.6829 - acc: 0.7206 - val_loss: 0.5344 - val_acc: 0.7902\n",
      "Epoch 4/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.4119 - acc: 0.8392 - val_loss: 0.3746 - val_acc: 0.8586\n",
      "Epoch 5/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.2382 - acc: 0.9143 - val_loss: 0.2749 - val_acc: 0.9025\n",
      "Epoch 6/10\n",
      "13413/13413 [==============================] - 17s 1ms/step - loss: 0.1271 - acc: 0.9591 - val_loss: 0.2186 - val_acc: 0.9298\n",
      "Epoch 7/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0794 - acc: 0.9750 - val_loss: 0.2107 - val_acc: 0.9320\n",
      "Epoch 8/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0588 - acc: 0.9830 - val_loss: 0.1943 - val_acc: 0.9374\n",
      "Epoch 9/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0432 - acc: 0.9868 - val_loss: 0.2022 - val_acc: 0.9423\n",
      "Epoch 10/10\n",
      "13413/13413 [==============================] - 18s 1ms/step - loss: 0.0408 - acc: 0.9882 - val_loss: 0.1843 - val_acc: 0.9526\n",
      "2235/2235 [==============================] - 7s 3ms/step\n",
      "Evaluation 0.9528642549961465\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 505us/step\n",
      "\n",
      "FOLD 5\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13414 samples, validate on 2234 samples\n",
      "Epoch 1/10\n",
      "13414/13414 [==============================] - 21s 2ms/step - loss: 1.0992 - acc: 0.3857 - val_loss: 1.0711 - val_acc: 0.4194\n",
      "Epoch 2/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 1.0120 - acc: 0.5044 - val_loss: 0.8912 - val_acc: 0.6128\n",
      "Epoch 3/10\n",
      "13414/13414 [==============================] - 17s 1ms/step - loss: 0.7159 - acc: 0.7200 - val_loss: 0.5451 - val_acc: 0.7762\n",
      "Epoch 4/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.3959 - acc: 0.8555 - val_loss: 0.3795 - val_acc: 0.8514\n",
      "Epoch 5/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.2095 - acc: 0.9301 - val_loss: 0.2844 - val_acc: 0.9006\n",
      "Epoch 6/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.1242 - acc: 0.9610 - val_loss: 0.2625 - val_acc: 0.9136\n",
      "Epoch 7/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0811 - acc: 0.9747 - val_loss: 0.2276 - val_acc: 0.9346\n",
      "Epoch 8/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0639 - acc: 0.9807 - val_loss: 0.2264 - val_acc: 0.9351\n",
      "Epoch 9/10\n",
      "13414/13414 [==============================] - 17s 1ms/step - loss: 0.0456 - acc: 0.9871 - val_loss: 0.3176 - val_acc: 0.9064\n",
      "Epoch 10/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0539 - acc: 0.9849 - val_loss: 0.2324 - val_acc: 0.9405\n",
      "2234/2234 [==============================] - 7s 3ms/step\n",
      "Evaluation 0.9407423250964394\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 498us/step\n",
      "\n",
      "FOLD 6\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13414 samples, validate on 2234 samples\n",
      "Epoch 1/10\n",
      "13414/13414 [==============================] - 22s 2ms/step - loss: 1.1053 - acc: 0.3487 - val_loss: 1.0744 - val_acc: 0.4154\n",
      "Epoch 2/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 1.0217 - acc: 0.4943 - val_loss: 0.9126 - val_acc: 0.5703\n",
      "Epoch 3/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.7763 - acc: 0.6587 - val_loss: 0.6285 - val_acc: 0.6943\n",
      "Epoch 4/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.5210 - acc: 0.7916 - val_loss: 0.4970 - val_acc: 0.8035\n",
      "Epoch 5/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.3308 - acc: 0.8776 - val_loss: 0.3292 - val_acc: 0.8688\n",
      "Epoch 6/10\n",
      "13414/13414 [==============================] - 19s 1ms/step - loss: 0.1966 - acc: 0.9337 - val_loss: 0.2577 - val_acc: 0.9078\n",
      "Epoch 7/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.1300 - acc: 0.9591 - val_loss: 0.2461 - val_acc: 0.9145\n",
      "Epoch 8/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0991 - acc: 0.9697 - val_loss: 0.2910 - val_acc: 0.8957\n",
      "Epoch 9/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0892 - acc: 0.9735 - val_loss: 0.2356 - val_acc: 0.9297\n",
      "Epoch 10/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0526 - acc: 0.9847 - val_loss: 0.2264 - val_acc: 0.9324\n",
      "2234/2234 [==============================] - 7s 3ms/step\n",
      "Evaluation 0.9327932037434912\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 483us/step\n",
      "\n",
      "FOLD 7\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 13414 samples, validate on 2234 samples\n",
      "Epoch 1/10\n",
      "13414/13414 [==============================] - 22s 2ms/step - loss: 1.1231 - acc: 0.3470 - val_loss: 1.0800 - val_acc: 0.4078\n",
      "Epoch 2/10\n",
      "13414/13414 [==============================] - 19s 1ms/step - loss: 1.0324 - acc: 0.4809 - val_loss: 0.9322 - val_acc: 0.5488\n",
      "Epoch 3/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.7945 - acc: 0.6644 - val_loss: 0.6086 - val_acc: 0.7525\n",
      "Epoch 4/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.4858 - acc: 0.8133 - val_loss: 0.4207 - val_acc: 0.8357\n",
      "Epoch 5/10\n",
      "13414/13414 [==============================] - 17s 1ms/step - loss: 0.2779 - acc: 0.9023 - val_loss: 0.2866 - val_acc: 0.8903\n",
      "Epoch 6/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.1620 - acc: 0.9464 - val_loss: 0.2231 - val_acc: 0.9288\n",
      "Epoch 7/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.1066 - acc: 0.9660 - val_loss: 0.2094 - val_acc: 0.9293\n",
      "Epoch 8/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0665 - acc: 0.9796 - val_loss: 0.1912 - val_acc: 0.9378\n",
      "Epoch 9/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0491 - acc: 0.9860 - val_loss: 0.2630 - val_acc: 0.9235\n",
      "Epoch 10/10\n",
      "13414/13414 [==============================] - 18s 1ms/step - loss: 0.0496 - acc: 0.9847 - val_loss: 0.2674 - val_acc: 0.9279\n",
      "Epoch 00010: early stopping\n",
      "2234/2234 [==============================] - 7s 3ms/step\n",
      "Evaluation 0.9278063640184202\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 1s 484us/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds2 = np.zeros((test.shape[0], 3))\n",
    "from sklearn.metrics import f1_score\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(\"FOLD\", i+1)\n",
    "    \n",
    "    print(\"Splitting the data into train and validation...\")\n",
    "    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n",
    "    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n",
    "    y_train = enc.transform(train[train[\"fold_id\"] != i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    y_val = enc.transform(train[train[\"fold_id\"] == i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "    model = build_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n",
    "              epochs=10, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n",
    "#     print(np.argmax(model.predict([val_seq, val_dense[dense_features]], batch_size=128, verbose=1),axis=1),y_val)\n",
    "    print('Evaluation',f1_score(np.argmax(model.predict([val_seq, val_dense[dense_features]], batch_size=128, verbose=1),axis=1),train[train[\"fold_id\"] == i][\"sentiment\"].values,average='macro'))\n",
    "    print(\"Predicting...\")\n",
    "    test_preds2 += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n",
    "    print()\n",
    "    \n",
    "test_preds2 /= NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_15 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13410 samples, validate on 2238 samples\n",
      "Epoch 1/10\n",
      "13410/13410 [==============================] - 11s 827us/step - loss: 1.0943 - acc: 0.3670 - val_loss: 1.0899 - val_acc: 0.3749\n",
      "Epoch 2/10\n",
      "13410/13410 [==============================] - 6s 474us/step - loss: 1.0331 - acc: 0.4661 - val_loss: 0.8952 - val_acc: 0.5594\n",
      "Epoch 3/10\n",
      "13410/13410 [==============================] - 6s 474us/step - loss: 0.8267 - acc: 0.6020 - val_loss: 0.6676 - val_acc: 0.7140\n",
      "Epoch 4/10\n",
      "13410/13410 [==============================] - 6s 472us/step - loss: 0.4946 - acc: 0.8081 - val_loss: 0.3725 - val_acc: 0.8552\n",
      "Epoch 5/10\n",
      "13410/13410 [==============================] - 6s 474us/step - loss: 0.2143 - acc: 0.9232 - val_loss: 0.2239 - val_acc: 0.9236\n",
      "Epoch 6/10\n",
      "13410/13410 [==============================] - 6s 474us/step - loss: 0.0929 - acc: 0.9723 - val_loss: 0.1147 - val_acc: 0.9678\n",
      "Epoch 7/10\n",
      "13410/13410 [==============================] - 6s 475us/step - loss: 0.0442 - acc: 0.9880 - val_loss: 0.0922 - val_acc: 0.9745\n",
      "Epoch 8/10\n",
      "13410/13410 [==============================] - 6s 476us/step - loss: 0.0300 - acc: 0.9919 - val_loss: 0.0774 - val_acc: 0.9790\n",
      "Epoch 9/10\n",
      "13410/13410 [==============================] - 6s 476us/step - loss: 0.0216 - acc: 0.9956 - val_loss: 0.0712 - val_acc: 0.9799\n",
      "Epoch 10/10\n",
      "13410/13410 [==============================] - 6s 477us/step - loss: 0.0113 - acc: 0.9973 - val_loss: 0.0786 - val_acc: 0.9839\n",
      "2238/2238 [==============================] - 2s 782us/step\n",
      "Evaluation 0.9841270845080903\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 172us/step\n",
      "\n",
      "FOLD 2\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_16 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_2 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13410 samples, validate on 2238 samples\n",
      "Epoch 1/10\n",
      "13410/13410 [==============================] - 12s 860us/step - loss: 1.0949 - acc: 0.3548 - val_loss: 1.0858 - val_acc: 0.3825\n",
      "Epoch 2/10\n",
      "13410/13410 [==============================] - 6s 474us/step - loss: 1.0434 - acc: 0.4256 - val_loss: 0.9736 - val_acc: 0.4951\n",
      "Epoch 3/10\n",
      "13410/13410 [==============================] - 6s 474us/step - loss: 0.7977 - acc: 0.6246 - val_loss: 0.5985 - val_acc: 0.7417\n",
      "Epoch 4/10\n",
      "13410/13410 [==============================] - 6s 475us/step - loss: 0.4519 - acc: 0.8255 - val_loss: 0.3348 - val_acc: 0.8704\n",
      "Epoch 5/10\n",
      "13410/13410 [==============================] - 6s 472us/step - loss: 0.1793 - acc: 0.9411 - val_loss: 0.1591 - val_acc: 0.9486\n",
      "Epoch 6/10\n",
      "13410/13410 [==============================] - 6s 472us/step - loss: 0.0813 - acc: 0.9770 - val_loss: 0.1201 - val_acc: 0.9647\n",
      "Epoch 7/10\n",
      "13410/13410 [==============================] - 6s 472us/step - loss: 0.0460 - acc: 0.9880 - val_loss: 0.1120 - val_acc: 0.9701\n",
      "Epoch 8/10\n",
      "13410/13410 [==============================] - 6s 472us/step - loss: 0.0390 - acc: 0.9910 - val_loss: 0.0687 - val_acc: 0.9839\n",
      "Epoch 9/10\n",
      "13410/13410 [==============================] - 6s 473us/step - loss: 0.0340 - acc: 0.9916 - val_loss: 0.0560 - val_acc: 0.9861\n",
      "Epoch 10/10\n",
      "13410/13410 [==============================] - 6s 471us/step - loss: 0.0237 - acc: 0.9955 - val_loss: 0.0444 - val_acc: 0.9884\n",
      "2238/2238 [==============================] - 2s 844us/step\n",
      "Evaluation 0.988577754362426\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 168us/step\n",
      "\n",
      "FOLD 3\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_17 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_3 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13413 samples, validate on 2235 samples\n",
      "Epoch 1/10\n",
      "13413/13413 [==============================] - 12s 889us/step - loss: 1.0950 - acc: 0.3595 - val_loss: 1.0890 - val_acc: 0.3714\n",
      "Epoch 2/10\n",
      "13413/13413 [==============================] - 6s 470us/step - loss: 1.0517 - acc: 0.4265 - val_loss: 0.9223 - val_acc: 0.5584\n",
      "Epoch 3/10\n",
      "13413/13413 [==============================] - 6s 472us/step - loss: 0.7923 - acc: 0.6258 - val_loss: 0.6241 - val_acc: 0.7047\n",
      "Epoch 4/10\n",
      "13413/13413 [==============================] - 6s 472us/step - loss: 0.5066 - acc: 0.7928 - val_loss: 0.3972 - val_acc: 0.8309\n",
      "Epoch 5/10\n",
      "13413/13413 [==============================] - 6s 473us/step - loss: 0.2422 - acc: 0.9199 - val_loss: 0.2013 - val_acc: 0.9298\n",
      "Epoch 6/10\n",
      "13413/13413 [==============================] - 6s 470us/step - loss: 0.1075 - acc: 0.9698 - val_loss: 0.1326 - val_acc: 0.9593\n",
      "Epoch 7/10\n",
      "13413/13413 [==============================] - 6s 471us/step - loss: 0.0519 - acc: 0.9881 - val_loss: 0.1220 - val_acc: 0.9620\n",
      "Epoch 8/10\n",
      "13413/13413 [==============================] - 6s 473us/step - loss: 0.0358 - acc: 0.9914 - val_loss: 0.0924 - val_acc: 0.9745\n",
      "Epoch 9/10\n",
      "13413/13413 [==============================] - 6s 472us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.1044 - val_acc: 0.9732\n",
      "Epoch 10/10\n",
      "13413/13413 [==============================] - 6s 473us/step - loss: 0.0197 - acc: 0.9957 - val_loss: 0.0888 - val_acc: 0.9781\n",
      "2235/2235 [==============================] - 2s 912us/step\n",
      "Evaluation 0.9784967976136391\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 168us/step\n",
      "\n",
      "FOLD 4\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_4 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13413 samples, validate on 2235 samples\n",
      "Epoch 1/10\n",
      "13413/13413 [==============================] - 12s 910us/step - loss: 1.0933 - acc: 0.3577 - val_loss: 1.0766 - val_acc: 0.3839\n",
      "Epoch 2/10\n",
      "13413/13413 [==============================] - 6s 472us/step - loss: 1.0063 - acc: 0.4663 - val_loss: 0.8535 - val_acc: 0.6000\n",
      "Epoch 3/10\n",
      "13413/13413 [==============================] - 6s 473us/step - loss: 0.7091 - acc: 0.6713 - val_loss: 0.6836 - val_acc: 0.6658\n",
      "Epoch 4/10\n",
      "13413/13413 [==============================] - 6s 477us/step - loss: 0.7306 - acc: 0.6944 - val_loss: 0.6734 - val_acc: 0.6676\n",
      "Epoch 5/10\n",
      "13413/13413 [==============================] - 6s 474us/step - loss: 0.5947 - acc: 0.7479 - val_loss: 0.5339 - val_acc: 0.8004\n",
      "Epoch 6/10\n",
      "13413/13413 [==============================] - 6s 474us/step - loss: 0.4066 - acc: 0.8611 - val_loss: 0.3664 - val_acc: 0.8586\n",
      "Epoch 7/10\n",
      "13413/13413 [==============================] - 6s 474us/step - loss: 0.2305 - acc: 0.9264 - val_loss: 0.2542 - val_acc: 0.9128\n",
      "Epoch 8/10\n",
      "13413/13413 [==============================] - 6s 474us/step - loss: 0.1295 - acc: 0.9637 - val_loss: 0.1834 - val_acc: 0.9409\n",
      "Epoch 9/10\n",
      "13413/13413 [==============================] - 6s 473us/step - loss: 0.0767 - acc: 0.9812 - val_loss: 0.1474 - val_acc: 0.9530\n",
      "Epoch 10/10\n",
      "13413/13413 [==============================] - 6s 474us/step - loss: 0.0555 - acc: 0.9862 - val_loss: 0.1297 - val_acc: 0.9602\n",
      "2235/2235 [==============================] - 2s 954us/step\n",
      "Evaluation 0.9610402282496125\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 168us/step\n",
      "\n",
      "FOLD 5\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_19 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_5 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13414 samples, validate on 2234 samples\n",
      "Epoch 1/10\n",
      "13414/13414 [==============================] - 12s 925us/step - loss: 1.0919 - acc: 0.3657 - val_loss: 1.0781 - val_acc: 0.4002\n",
      "Epoch 2/10\n",
      "13414/13414 [==============================] - 6s 473us/step - loss: 0.9931 - acc: 0.4725 - val_loss: 0.8180 - val_acc: 0.5837\n",
      "Epoch 3/10\n",
      "13414/13414 [==============================] - 6s 473us/step - loss: 0.7542 - acc: 0.6129 - val_loss: 0.6998 - val_acc: 0.6683\n",
      "Epoch 4/10\n",
      "13414/13414 [==============================] - 6s 474us/step - loss: 0.5952 - acc: 0.7025 - val_loss: 0.5659 - val_acc: 0.7243\n",
      "Epoch 5/10\n",
      "13414/13414 [==============================] - 6s 472us/step - loss: 0.4184 - acc: 0.8157 - val_loss: 0.3722 - val_acc: 0.8568\n",
      "Epoch 6/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.2319 - acc: 0.9161 - val_loss: 0.2373 - val_acc: 0.9150\n",
      "Epoch 7/10\n",
      "13414/13414 [==============================] - 6s 474us/step - loss: 0.1411 - acc: 0.9513 - val_loss: 0.2685 - val_acc: 0.9042\n",
      "Epoch 8/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.0715 - acc: 0.9810 - val_loss: 0.1760 - val_acc: 0.9593\n",
      "Epoch 9/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.0522 - acc: 0.9871 - val_loss: 0.1463 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.0383 - acc: 0.9914 - val_loss: 0.1319 - val_acc: 0.9687\n",
      "2234/2234 [==============================] - 2s 1ms/step\n",
      "Evaluation 0.9691650450020651\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 172us/step\n",
      "\n",
      "FOLD 6\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_20 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13414 samples, validate on 2234 samples\n",
      "Epoch 1/10\n",
      "13414/13414 [==============================] - 13s 961us/step - loss: 1.0946 - acc: 0.3633 - val_loss: 1.0856 - val_acc: 0.3796\n",
      "Epoch 2/10\n",
      "13414/13414 [==============================] - 6s 473us/step - loss: 1.0296 - acc: 0.4513 - val_loss: 0.8726 - val_acc: 0.5936\n",
      "Epoch 3/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.7596 - acc: 0.6382 - val_loss: 0.5806 - val_acc: 0.7274\n",
      "Epoch 4/10\n",
      "13414/13414 [==============================] - 6s 474us/step - loss: 0.5019 - acc: 0.7920 - val_loss: 0.3473 - val_acc: 0.8693\n",
      "Epoch 5/10\n",
      "13414/13414 [==============================] - 6s 474us/step - loss: 0.2587 - acc: 0.9143 - val_loss: 0.2258 - val_acc: 0.9360\n",
      "Epoch 6/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.1111 - acc: 0.9703 - val_loss: 0.1292 - val_acc: 0.9651\n",
      "Epoch 7/10\n",
      "13414/13414 [==============================] - 6s 475us/step - loss: 0.0582 - acc: 0.9857 - val_loss: 0.0961 - val_acc: 0.9736\n",
      "Epoch 8/10\n",
      "13414/13414 [==============================] - 6s 476us/step - loss: 0.0369 - acc: 0.9917 - val_loss: 0.0838 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      "13414/13414 [==============================] - 6s 477us/step - loss: 0.0268 - acc: 0.9940 - val_loss: 0.0892 - val_acc: 0.9812\n",
      "Epoch 10/10\n",
      "13414/13414 [==============================] - 6s 474us/step - loss: 0.0217 - acc: 0.9960 - val_loss: 0.0893 - val_acc: 0.9803\n",
      "Epoch 00010: early stopping\n",
      "2234/2234 [==============================] - 3s 1ms/step\n",
      "Evaluation 0.9804603473070818\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 165us/step\n",
      "\n",
      "FOLD 7\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "embedding_21 (Embedding)     (None, 350, 300)          13192200  \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 350, 256)          440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 350, 128)          164864    \n",
      "_________________________________________________________________\n",
      "attention_7 (Attention)      (None, 128)               478       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 13,806,313\n",
      "Trainable params: 13,806,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Train on 13414 samples, validate on 2234 samples\n",
      "Epoch 1/10\n",
      "13414/13414 [==============================] - 14s 1ms/step - loss: 1.0941 - acc: 0.3598 - val_loss: 1.0838 - val_acc: 0.3765\n",
      "Epoch 2/10\n",
      "13414/13414 [==============================] - 6s 471us/step - loss: 1.0370 - acc: 0.4502 - val_loss: 0.9181 - val_acc: 0.5461\n",
      "Epoch 3/10\n",
      "13414/13414 [==============================] - 6s 470us/step - loss: 0.8058 - acc: 0.6429 - val_loss: 0.5953 - val_acc: 0.7498\n",
      "Epoch 4/10\n",
      "13414/13414 [==============================] - 6s 471us/step - loss: 0.4132 - acc: 0.8413 - val_loss: 0.2677 - val_acc: 0.9038\n",
      "Epoch 5/10\n",
      "13414/13414 [==============================] - 6s 471us/step - loss: 0.1598 - acc: 0.9470 - val_loss: 0.2149 - val_acc: 0.9252\n",
      "Epoch 6/10\n",
      "13414/13414 [==============================] - 6s 471us/step - loss: 0.0972 - acc: 0.9710 - val_loss: 0.0990 - val_acc: 0.9642\n",
      "Epoch 7/10\n",
      "13414/13414 [==============================] - 6s 471us/step - loss: 0.0456 - acc: 0.9870 - val_loss: 0.0734 - val_acc: 0.9785\n",
      "Epoch 8/10\n",
      "13414/13414 [==============================] - 6s 472us/step - loss: 0.0281 - acc: 0.9925 - val_loss: 0.0660 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      "13414/13414 [==============================] - 6s 470us/step - loss: 0.0252 - acc: 0.9940 - val_loss: 0.0601 - val_acc: 0.9848\n",
      "Epoch 10/10\n",
      "13414/13414 [==============================] - 6s 471us/step - loss: 0.0158 - acc: 0.9960 - val_loss: 0.0620 - val_acc: 0.9848\n",
      "2234/2234 [==============================] - 3s 1ms/step\n",
      "Evaluation 0.9849693912156958\n",
      "Predicting...\n",
      "2081/2081 [==============================] - 0s 167us/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds3 = np.zeros((test.shape[0], 3))\n",
    "from sklearn.metrics import f1_score\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(\"FOLD\", i+1)\n",
    "    \n",
    "    print(\"Splitting the data into train and validation...\")\n",
    "    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n",
    "    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n",
    "    y_train = enc.transform(train[train[\"fold_id\"] != i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    y_val = enc.transform(train[train[\"fold_id\"] == i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "#     model = build_model_only()\n",
    "    model = buildAtt_layer()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit(train_seq, y_train, validation_data=(val_seq, y_val),\n",
    "              epochs=10, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n",
    "#     print(np.argmax(model.predict([val_seq, val_dense[dense_features]], batch_size=128, verbose=1),axis=1),y_val)\n",
    "    print('Evaluation',f1_score(np.argmax(model.predict(val_seq, batch_size=512, verbose=1),axis=1),train[train[\"fold_id\"] == i][\"sentiment\"].values,average='macro'))\n",
    "    print(\"Predicting...\")\n",
    "    test_preds3 += model.predict(test_seq, batch_size=1024, verbose=1)\n",
    "    print()\n",
    "    \n",
    "test_preds3 /= NUM_FOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfc473082e4f349ceb59a6df2e96f173f762195c"
   },
   "source": [
    "**Making submission...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds=(test_preds1+test_preds2+test_preds3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class with the highest probability as prediction...\n",
      "(2924, 2)\n",
      "Make the submission ready...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    2472\n",
       "1     270\n",
       "0     182\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Select the class with the highest probability as prediction...\")\n",
    "test[\"sentiment\"] = test_preds.argmax(axis=1)\n",
    "# test.loc[test[\"sentiment\"].isnull(), \"pred\"]\n",
    "test_common['sentiment']=2\n",
    "sub=test[[\"unique_hash\", \"sentiment\"]]\n",
    "sub=sub.append(test_common[[\"unique_hash\", \"sentiment\"]],ignore_index=True)\n",
    "print(sub.shape)\n",
    "\n",
    "print(\"Make the submission ready...\")\n",
    "sub[\"sentiment\"] = sub[\"sentiment\"].astype(int)\n",
    "sub.to_csv(\"submissionkv10stacked.csv\", index=False)\n",
    "sub.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class with the highest probability as prediction...\n",
      "(2924, 2)\n",
      "Make the submission ready...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    2535\n",
       "1     223\n",
       "0     166\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Select the class with the highest probability as prediction...\")\n",
    "test[\"sentiment\"] = test_preds1.argmax(axis=1)\n",
    "# test.loc[test[\"sentiment\"].isnull(), \"pred\"]\n",
    "test_common['sentiment']=2\n",
    "sub=test[[\"unique_hash\", \"sentiment\"]]\n",
    "sub=sub.append(test_common[[\"unique_hash\", \"sentiment\"]],ignore_index=True)\n",
    "print(sub.shape)\n",
    "\n",
    "print(\"Make the submission ready...\")\n",
    "sub[\"sentiment\"] = sub[\"sentiment\"].astype(int)\n",
    "sub.to_csv(\"submissionkv10CNN.csv\", index=False)\n",
    "sub.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class with the highest probability as prediction...\n",
      "(2924, 2)\n",
      "Make the submission ready...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    2234\n",
       "1     370\n",
       "0     320\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Select the class with the highest probability as prediction...\")\n",
    "test[\"sentiment\"] = test_preds2.argmax(axis=1)\n",
    "# test.loc[test[\"sentiment\"].isnull(), \"pred\"]\n",
    "test_common['sentiment']=2\n",
    "sub=test[[\"unique_hash\", \"sentiment\"]]\n",
    "sub=sub.append(test_common[[\"unique_hash\", \"sentiment\"]],ignore_index=True)\n",
    "print(sub.shape)\n",
    "\n",
    "print(\"Make the submission ready...\")\n",
    "sub[\"sentiment\"] = sub[\"sentiment\"].astype(int)\n",
    "sub.to_csv(\"submissionkv10withFeat.csv\", index=False)\n",
    "sub.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class with the highest probability as prediction...\n",
      "(2924, 2)\n",
      "Make the submission ready...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    2432\n",
       "1     302\n",
       "0     190\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Select the class with the highest probability as prediction...\")\n",
    "test[\"sentiment\"] = test_preds3.argmax(axis=1)\n",
    "# test.loc[test[\"sentiment\"].isnull(), \"pred\"]\n",
    "test_common['sentiment']=2\n",
    "sub=test[[\"unique_hash\", \"sentiment\"]]\n",
    "sub=sub.append(test_common[[\"unique_hash\", \"sentiment\"]],ignore_index=True)\n",
    "print(sub.shape)\n",
    "\n",
    "print(\"Make the submission ready...\")\n",
    "sub[\"sentiment\"] = sub[\"sentiment\"].astype(int)\n",
    "sub.to_csv(\"submissionkv10ATT.csv\", index=False)\n",
    "sub.sentiment.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
