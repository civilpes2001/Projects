# -*- coding: utf-8 -*-
"""Forest Cover Classification: Weekend Hackathon #12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ogp_SX138QAIsAibKUUB9X7U9uZK21rW
"""

!wget https://machinehack-be.s3.amazonaws.com/forest_cover_classification_weekend_hackathon_12/Forest_Cover_participants_Data.zip

!unzip Forest_Cover_participants_Data.zip

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

train = pd.read_csv('Forest_Cover_participants_Data/train.csv')
test = pd.read_csv('Forest_Cover_participants_Data/test.csv')
sub = pd.read_csv('Forest_Cover_participants_Data/sample_submission.csv')

train.tail(5)

test.head(5)

train.isnull().sum(),test.isnull().sum(),train.shape,test.shape,train.dtypes

import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)

train['Cover_Type']=train['Cover_Type']-1
import seaborn as sns
sns.countplot(train['Cover_Type'])

train_df=train
test_df=test

X = train_df.drop(labels=['Cover_Type'], axis=1)
y = train_df['Cover_Type'].values

from sklearn.model_selection import train_test_split
X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)

X_train.shape, y_train.shape, X_cv.shape, y_cv.shape

from sklearn.metrics import log_loss

import lightgbm as lgb
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_cv, label=y_cv)

param = {'objective': 'multiclass',
         'num_class': 7,
         'boosting': 'gbdt',  
         'metric': 'multi_logloss',
         'learning_rate': 0.1, 
         'num_iterations': 2000,
         'num_leaves': 100,
         'max_depth': -1,
         'min_data_in_leaf': 15,
         'bagging_fraction':0.9,
         'bagging_freq': 1,
         'feature_fraction': 0.7,
         'lambda_l2': 0.8,
         'min_data_per_group': 75,
         'max_bin': 255,
         'is_unbalance':True
         }

clf = lgb.train(params=param, 
                early_stopping_rounds=200,
                verbose_eval=100,
                train_set=train_data,
                valid_sets=[test_data])

y_pred = clf.predict(X_cv)

log_loss(y_cv, y_pred)

feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(), X.columns), reverse=True)[:], columns=['Value','Feature'])
plt.figure(figsize=(20,20))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False))
plt.title('LightGBM Features')
plt.tight_layout()
plt.show()

feature_imp.loc[feature_imp.Value >= 0].sort_values(by=['Value'], ascending = False)

Xtest = test_df

from sklearn.model_selection import KFold, StratifiedKFold

errlgb = []
y_pred_totlgb = []

fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=123456789)

for train_index, test_index in fold.split(X, y):
    
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test)
    
    clf = lgb.train(params=param, 
                     early_stopping_rounds=200,
                     verbose_eval=100,
                     train_set=train_data,
                     valid_sets=[test_data])

    y_pred = clf.predict(X_test)
    print("Log Loss: ", (log_loss(y_test, y_pred)))
    
    errlgb.append(log_loss(y_test, y_pred))
    p = clf.predict(Xtest)
    y_pred_totlgb.append(p)

np.mean(errlgb,0)

sub.columns

y_pred = np.mean(y_pred_totlgb,0)

submission = pd.DataFrame(data=y_pred, columns=sub.columns)
submission.head()

submission.to_csv('alpha3.csv',index=False)

from google.colab import files
files.download('alpha1.csv')

from xgboost import XGBClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
xgb = XGBClassifier(random_state=42)
xgb.fit(X_train, y_train)
y_pred = xgb.predict_proba(X_cv)

log_loss(y_cv, y_pred)

Xtest = test_df

err = []
y_pred_tot = []

fold = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)

for train_index, test_index in fold.split(X, y):
    
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = y[train_index], y[test_index]

    xgb = XGBClassifier(random_state=42)
    xgb.fit(X_train, y_train)
    y_pred = xgb.predict_proba(X_test)
    
    print("Log Loss:", log_loss(y_test, y_pred))

    err.append(log_loss(y_test, y_pred))
    p = xgb.predict_proba(Xtest)
    y_pred_tot.append(p)

np.mean(err, 0)

np.mean(y_pred_tot, 0)

y_pred = np.mean(y_pred_tot, 0)

sub = pd.DataFrame(y_pred)
sub.head(5)

sub.to_csv('s1.csv', index=False)



from sklearn.model_selection import train_test_split
X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.15, random_state=101, stratify=y)

import lightgbm as lgb
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_cv, label=y_cv)

param = {'objective': 'multiclass',
         'num_class': 7,
         'boosting': 'gbdt',  
         'metric': 'multi_logloss',
         'learning_rate': 0.1, 
         'num_iterations': 1000,
         'num_leaves': 100,
         'max_depth': -1,
         'min_data_in_leaf': 15,
         'bagging_fraction':0.9,
         'bagging_freq': 1,
         'feature_fraction': 0.7,
         'lambda_l2': 0.8,
         'min_data_per_group': 75,
         'max_bin': 255,
         'is_unbalance':False
         }

clf = lgb.train(params=param, 
                early_stopping_rounds=200,
                verbose_eval=100,
                train_set=train_data,
                valid_sets=[test_data])

y_pred = clf.predict(X_cv)

Xtest = test_df

from sklearn.model_selection import KFold, StratifiedKFold

errlgb = []
y_pred_totlgb = []

fold = StratifiedKFold(n_splits=10, shuffle=False, random_state=123456789)

for train_index, test_index in fold.split(X, y):
    
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test)
    param = {'objective': 'multiclass',
         'num_class': 7,
         'boosting': 'gbdt',  
         'metric': 'multi_logloss',
         'learning_rate': 0.2, 
         'num_iterations': 2000,
         'num_leaves': 100,
         'max_depth': -1,
         'min_data_in_leaf': 15,
         'bagging_fraction':0.9,
         'bagging_freq': 1,
         'feature_fraction': 0.7,
         'lambda_l2': 0.8,
         'min_data_per_group': 75,
         'max_bin': 255,
         'is_unbalance':False
         }
    clf = lgb.train(params=param,
                     early_stopping_rounds=200,
                     verbose_eval=100,
                     train_set=train_data,
                     valid_sets=[test_data])

    y_pred = clf.predict(X_test)
    print("Log Loss: ", (log_loss(y_test, y_pred)))
    
    errlgb.append(log_loss(y_test, y_pred))
    p = clf.predict(Xtest)
    y_pred_totlgb.append(p)

np.mean(errlgb,0)

sub.columns

y_pred = np.mean(y_pred_totlgb,0)

submission = pd.DataFrame(data=y_pred, columns=sub.columns)
submission.head()

submission.to_csv('final.csv', index=False)

from google.colab import files
files.download('final.csv')